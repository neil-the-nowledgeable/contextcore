 # Export Enrichment Plan

**Goal**: Enrich the `contextcore manifest export` command output so that downstream consumers (plan ingestion, artisan workflow) receive the full contract data described in the [Export Pipeline Analysis Guide](../docs/EXPORT_PIPELINE_ANALYSIS_GUIDE.md), eliminating the need for downstream systems to re-parse the artifact manifest or re-derive information that the export already computed.

**Motivation**: Analysis of the current artisan-context-seed.json for the wayfinder project reveals that while onboarding metadata is structurally rich (parameter_sources, semantic_conventions, checksums), several high-value data fields are computed during export but never surfaced in `onboarding-metadata.json`. Downstream tasks—especially DESIGN and IMPLEMENT phases—are operating with incomplete contract data, leading to miscalibration, re-derivation, and missed context.

**Principle**: "Each system trusts the output of the one before it." (Guide §6). A defect or omission in the export cascades silently through ingestion into the artisan build.

---

## Changes Summary

All changes are in the ContextCore repo.  
Primary file: `src/contextcore/utils/onboarding.py` (`build_onboarding_metadata()`).  
Secondary file: `src/contextcore/models/manifest_v2.py` (one artifact type fix).

No changes to the CLI flags or `manifest.py` command wiring—all enhancements are additive fields in the onboarding metadata output.

---

## Change 1: Add `derivation_rules` to Onboarding Metadata

**Status**: Data exists in model, not surfaced.

**Problem**: Each `ArtifactSpec` has a `derived_from: List[DerivationRule]` field populated during `generate_artifact_manifest()`. Six of seven artifact types have real derivation rules (e.g., `criticality → P1 alert severity`, `availability → SLO target`). These rules are the design contract—the Guide §4 Phase 3 says "the artifact manifest's derivation rules become the design spec for each artifact." But `build_onboarding_metadata()` never reads `derived_from`.

**What the DESIGN phase needs**: For each artifact type, the mapping from business metadata field to artifact property, including the transformation logic and rationale. This eliminates the need for the LLM to guess how criticality maps to alert severity.

**Change**: In `build_onboarding_metadata()`, after building `artifact_types`, iterate over artifacts and collect `derived_from` rules, grouped by artifact type. Add as a `derivation_rules` key in onboarding metadata.

**File**: `src/contextcore/utils/onboarding.py`

**Location**: After the `artifact_types` loop (currently ends ~line 210), before the `semantic_conventions` block.

**New field structure**:

```json
{
  "derivation_rules": {
    "dashboard": [
      {
        "property": "panels.availability",
        "sourceField": "spec.requirements.availability",
        "transformation": "SLO threshold: 99.9%",
        "rationale": "Dashboard shows availability against SLO target"
      }
    ],
    "prometheus_rule": [
      {
        "property": "alertSeverity",
        "sourceField": "spec.business.criticality",
        "transformation": "critical → P1",
        "rationale": null
      }
    ]
  }
}
```

**Implementation sketch**:

```python
# ── Derivation rules per artifact type (Guide §4 Phase 3) ──────────
# Surfaces the derived_from rules so plan ingestion and artisan DESIGN
# have the concrete business→artifact mappings without re-parsing the
# full artifact manifest.
derivation_rules: Dict[str, List[Dict[str, Any]]] = {}
for artifact in artifact_manifest.artifacts:
    art_type = artifact.type.value
    if artifact.derived_from:
        if art_type not in derivation_rules:
            derivation_rules[art_type] = []
        for rule in artifact.derived_from:
            rule_dict = rule.model_dump(
                by_alias=True, exclude_none=True, mode="json"
            )
            # Deduplicate: same property+sourceField across targets
            if rule_dict not in derivation_rules[art_type]:
                derivation_rules[art_type].append(rule_dict)

if derivation_rules:
    result["derivation_rules"] = derivation_rules
```

**Tests**: Add a unit test in `tests/test_onboarding.py` that creates an `ArtifactManifest` with `derived_from` rules and asserts they appear in the output under `derivation_rules`.

---

## Change 2: Add `objectives` to Onboarding Metadata

**Status**: Data exists in model, not surfaced.

**Problem**: `ArtifactManifest.objectives` contains strategic objectives with key results including `metric_key`, `unit`, `target`, `operator`, `baseline`, and `window`. These are populated by `generate_artifact_manifest()` from the `.contextcore.yaml` strategy section. Dashboard generation (PI-019, PI-020) needs these to produce panels tied to real business KPIs. Currently, objectives are in `architectural_context` at the plan ingestion level but NOT in the structured onboarding metadata where artifact-level consumers can reliably find them.

**Change**: Add `objectives` to onboarding metadata by model-dumping `artifact_manifest.objectives`.

**File**: `src/contextcore/utils/onboarding.py`

**Location**: After the `guidance` block (~line 251), before the checksum section.

**Implementation sketch**:

```python
# ── Strategic objectives for dashboard panel generation ──────────
if artifact_manifest.objectives:
    result["objectives"] = [
        obj.model_dump(by_alias=True, exclude_none=True, mode="json")
        for obj in artifact_manifest.objectives
    ]
```

**New field structure**:

```json
{
  "objectives": [
    {
      "id": "OBJ-PERFORMANCE",
      "description": "Maintain sub-500ms P99 latency",
      "keyResults": [
        {
          "metricKey": "contextcore_latency_p99",
          "unit": "ms",
          "target": "500",
          "operator": "<=",
          "baseline": null,
          "window": "30d"
        }
      ]
    }
  ]
}
```

**Tests**: Assert `objectives` appears in output when artifact manifest has objectives populated.

---

## Change 3: Add `artifact_dependency_graph` to Onboarding Metadata

**Status**: Data exists in model, not surfaced.

**Problem**: Each `ArtifactSpec` has a `depends_on: List[str]` field establishing generation ordering (e.g., prometheus-rules depends on service-monitor, notification-policy depends on prometheus-rules). This ordering information is NOT surfaced, so downstream task sequencing relies entirely on the plan's dependency chain, which may not reflect artifact-level dependencies.

**Change**: Build an adjacency list from artifact `depends_on` fields and add as `artifact_dependency_graph`.

**File**: `src/contextcore/utils/onboarding.py`

**Location**: After the `objectives` block, before checksums.

**Implementation sketch**:

```python
# ── Artifact dependency graph for generation ordering ──────────
artifact_deps: Dict[str, List[str]] = {}
for artifact in artifact_manifest.artifacts:
    if artifact.depends_on:
        artifact_deps[artifact.id] = list(artifact.depends_on)

if artifact_deps:
    result["artifact_dependency_graph"] = artifact_deps
```

**New field structure**:

```json
{
  "artifact_dependency_graph": {
    "wayfinder_core-prometheus-rules": ["wayfinder_core-service-monitor"],
    "wayfinder_core-notification-policy": ["wayfinder_core-prometheus-rules"],
    "wayfinder_core-runbook": ["wayfinder_core-prometheus-rules"]
  }
}
```

**Tests**: Assert graph is populated when artifacts have `depends_on`.

---

## Change 4: Add `resolved_artifact_parameters` to Onboarding Metadata

**Status**: Data exists in model, not surfaced at the concrete level.

**Problem**: The current onboarding metadata has `parameter_sources` at the type level—generic mappings like `"alertSeverity": "manifest.spec.business.criticality (→ severity map)"`. But the artifact manifest has the **resolved concrete values** per artifact (e.g., `alertSeverity: P1`, `availabilityThreshold: 99.9`, `namespace: observability`). The DESIGN phase needs concrete values, not generic source pointers.

**Change**: Extract resolved parameter values per artifact and add as `resolved_artifact_parameters`.

**File**: `src/contextcore/utils/onboarding.py`

**Location**: After `artifact_dependency_graph`, before checksums.

**Implementation sketch**:

```python
# ── Resolved parameters per artifact (concrete values) ──────────
# Surfaces the actual computed parameter values so downstream consumers
# don't re-derive from generic parameter_sources.
resolved_params: Dict[str, Dict[str, Any]] = {}
for artifact in artifact_manifest.artifacts:
    if artifact.parameters:
        params_dict = {
            k: v for k, v in artifact.parameters.items()
            if v is not None
        }
        if params_dict:
            resolved_params[artifact.id] = params_dict

if resolved_params:
    result["resolved_artifact_parameters"] = resolved_params
```

**New field structure**:

```json
{
  "resolved_artifact_parameters": {
    "wayfinder_core-prometheus-rules": {
      "alertSeverity": "P1",
      "availabilityThreshold": "99.9",
      "latencyThreshold": "500ms",
      "throughput": "1000rps"
    },
    "wayfinder_core-service-monitor": {
      "metricsInterval": "30s",
      "namespace": "observability"
    }
  }
}
```

**Tests**: Assert resolved parameters appear when artifacts have `parameters` populated.

---

## Change 5: Add `open_questions` to Onboarding Metadata

**Status**: Data exists in guidance section of v2 manifest, not surfaced in onboarding.

**Problem**: The `.contextcore.yaml` v2 manifest can contain open questions in `guidance.questions` (e.g., `Q-DASHBOARD-FORMAT: "Should dashboard generation target Grafana provisioning format or Grafana API format?"` at `priority: high`). These are high-value signals for the DESIGN phase—Q-DASHBOARD-FORMAT directly affects PI-019/PI-020 implementation. The `guidance` block is already in onboarding, but it's a raw model dump. Open questions should be surfaced explicitly so plan ingestion can inject them as design constraints.

**Change**: Extract open questions from `guidance.questions` where `status == "open"` and add as a top-level `open_questions` list.

**File**: `src/contextcore/utils/onboarding.py`

**Location**: After the `guidance` block (~line 251).

**Implementation sketch**:

```python
# ── Open questions for design decision surfacing ──────────
if artifact_manifest.guidance and hasattr(artifact_manifest.guidance, 'questions'):
    open_qs = [
        q.model_dump(by_alias=True, exclude_none=True, mode="json")
        for q in (artifact_manifest.guidance.questions or [])
        if getattr(q, 'status', 'open') == 'open'
    ]
    if open_qs:
        result["open_questions"] = open_qs
```

**Tests**: Assert `open_questions` appears when guidance has open questions.

---

## Change 6: Fix Loki Rules Missing Derivation Rules

**Status**: Bug in artifact manifest generation.

**Problem**: In `manifest_v2.py`, the `loki_rule` artifact type generation (approximately lines 840-866 in `_generate_artifacts_for_target`) does not populate `derived_from`, unlike all other 6 artifact types. This means Loki rules have no derivation audit trail.

**Change**: Add `derived_from` to the Loki Rule `ArtifactSpec` construction.

**File**: `src/contextcore/models/manifest_v2.py`

**Location**: In the Loki rule artifact generation block, add `derived_from` similar to the other artifact types.

**Implementation sketch**:

```python
derived_from=[
    DerivationRule(
        property="logSelectors",
        source_field="spec.targets[].name",
        transformation=f"{{job=\"{target_name}\"}}",
        rationale="Log selectors derived from service target name",
    ),
],
```

**Tests**: Assert the generated loki_rule artifact has non-empty `derived_from`.

---

## Change 7: Commit Existing `file_ownership` Change

**Status**: Implemented but uncommitted (47 lines added, visible in `git diff`).

**Action**: Commit the existing `file_ownership` code in `onboarding.py` (lines 275-320) as part of this batch. This was implemented in the previous session's defense-in-depth work.

---

## Downstream Changes (startd8-sdk, Separate PR)

These are NOT part of the ContextCore export changes but are the consuming side. Listed here for completeness—they'll be done after the export re-run:

1. **Plan Ingestion — Carry `coverage` into seed**: `_phase_emit` should copy `onboarding.coverage` into the seed so artisan agents know the actual project state (0% coverage, 7 gaps).

2. **Plan Ingestion — Artifact-type-aware design calibration**: `_derive_design_calibration` should use artifact type as a signal per Guide §5 Principle 5:

   | Artifact Type | Expected Depth |
   |---|---|
   | ServiceMonitor | brief (≤50 LOC) |
   | PrometheusRule | standard (51-150 LOC) |
   | Dashboard | comprehensive (>150 LOC) |
   | SLO Definition | standard |
   | Runbook | standard-comprehensive |
   | Loki Rule | standard |
   | Notification Policy | standard |

3. **Plan Ingestion — Surface open questions**: Inject `open_questions` from onboarding as explicit `design_constraints` in the seed tasks that they apply to.

4. **Plan Ingestion — Inject `derivation_rules`**: Embed per-task derivation rules from onboarding into the seed's task context, so the DESIGN phase prompt includes the concrete business→artifact mapping.

---

## Execution Order

1. Implement Changes 1-6 in `onboarding.py` and `manifest_v2.py`
2. Change 7: Commit all (including pre-existing `file_ownership`)
3. Run `contextcore manifest export` with `--emit-provenance` for wayfinder
4. Verify new fields in `onboarding-metadata.json`
5. Re-run plan ingestion in startd8-sdk
6. Verify enriched seed
7. Build PI-001 with `--adopt-prior`, then PI-002+

## Testing

- Existing `tests/test_onboarding.py` tests should continue to pass (additive changes only)
- Add 6 new test cases: one per new field (`derivation_rules`, `objectives`, `artifact_dependency_graph`, `resolved_artifact_parameters`, `open_questions`, loki rule `derived_from`)
- Run full test suite: `pytest tests/ -v`

## Risk

Low. All changes are additive—new optional keys in the onboarding metadata JSON. No existing keys are modified or removed. Downstream consumers that don't read the new keys are unaffected.

---

## Appendix: Iterative Review Log (Applied / Rejected Suggestions)

This appendix is intentionally **append-only**. New reviewers (human or model) should add suggestions to Appendix C, and then once validated, record the final disposition in Appendix A (applied) or Appendix B (rejected with rationale).

### Reviewer Instructions (for humans + models)

- **Before suggesting changes**: Scan Appendix A and Appendix B first. Do **not** re-suggest items already applied or explicitly rejected.
- **When proposing changes**: Append them to Appendix C using a unique suggestion ID (`R{round}-S{n}`).
- **When endorsing prior suggestions**: If you agree with an untriaged suggestion from a prior round, list it in an **Endorsements** section after your suggestion table. This builds consensus signal — suggestions endorsed by multiple reviewers should be prioritized during triage.
- **When validating**: For each suggestion, append a row to Appendix A (if applied) or Appendix B (if rejected) referencing the suggestion ID. Endorsement counts inform priority but do not auto-apply suggestions.
- **If rejecting**: Record **why** (specific rationale) so future models don't re-propose the same idea.

### Areas Substantially Addressed

- **architecture**: 3 suggestions applied (R1-S3, R1-S8, R2-S2)
- **data**: 4 suggestions applied (R5-S1, R1-S6, R1-S14, R2-S10)
- **interfaces**: 5 suggestions applied (R5-S2, R5-S11, R1-S5, R3-S4, R3-S6)
- **ops**: 6 suggestions applied (R5-S5, R1-S7, R1-S13, R3-S10, R4-S2, R4-S5)
- **risks**: 6 suggestions applied (R5-S12, R1-S4, R2-S1, R2-S5, R3-S15, R6-S1)
- **security**: 4 suggestions applied (R6-S2, R3-S1, R3-S2, R3-S14)
- **validation**: 7 suggestions applied (R5-S3, R5-S8, R6-S1, R1-S2, R1-S10, R1-S15, R2-S6)

### Areas Needing Further Review

All areas have reached the substantially addressed threshold.

### Appendix A: Applied Suggestions

| ID | Suggestion | Source | Implementation / Validation Notes | Date |
|----|------------|--------|----------------------------------|------|
| R1-S2 | Add an integration test that round-trips a real .contextcore.yaml through the full export pipeline and asserts all 5 new fields are structurally correct. | claude-4 (claude-opus-4-6) | The plan only specifies unit tests with hand-constructed objects. An integration test is essential to catch pipeline-level failures (e.g., model mismatches, field population gaps) that unit tests with mocked data will miss. This directly validates the stated principle that defects cascade silently. | 2026-02-13 15:54:51 UTC |
| R1-S3 | Define an explicit Pydantic model or JSON Schema for the onboarding metadata output instead of ad-hoc dict construction. | claude-4 (claude-opus-4-6) | The output is growing to 10+ top-level keys with complex nested structures. A typed output model provides compile-time safety, self-documenting contracts, serialization consistency, and enables auto-generated schema for downstream consumers. This is a high-value architectural improvement. | 2026-02-13 15:54:51 UTC |
| R1-S4 | Replace the O(n²) dict-in-list deduplication in Change 1 with a set-based approach using a hashable key tuple. | claude-4 (claude-opus-4-6) | Dict equality comparison is order-dependent in practice and O(n²). Using a (property, sourceField) tuple as a dedup key is both correct and performant. This is a real bug risk given that Pydantic's model_dump output ordering can vary. | 2026-02-13 15:54:51 UTC |
| R1-S5 | Document the downstream contract for each new field, specifying consumers, absent-vs-empty semantics, and cardinality constraints. | claude-4 (claude-opus-4-6) | The plan lists 5 new fields and 4 downstream changes but never formally links them. Without explicit contracts, it's ambiguous whether an absent field is a bug or acceptable, undermining the plan's own 'trust the output' principle. | 2026-02-13 15:54:51 UTC |
| R1-S6 | Sanitize or filter artifact parameters before including them in resolved_artifact_parameters to prevent leaking secrets or sensitive data. | claude-4 (claude-opus-4-6) | Change 4 dumps all non-None parameter values indiscriminately. If .contextcore.yaml contains parameters like database_password or auth_token, these would be surfaced in exported JSON and potentially committed to repos or passed to LLM contexts. A parameter allowlist or sensitive flag is a necessary safeguard. | 2026-02-13 15:54:51 UTC |
| R1-S7 | Ensure the existing checksum computation covers the 5 new fields to maintain integrity guarantees. | claude-4 (claude-opus-4-6) | If checksums are computed before the new fields are added, tampering or corruption in derivation_rules, objectives, etc. would go undetected. This is a straightforward ordering concern that should be verified and documented. | 2026-02-13 15:54:51 UTC |
| R1-S8 | Replace hasattr/getattr guards on guidance.questions with proper Optional field typing in the Pydantic model. | claude-4 (claude-opus-4-6) | Using hasattr on a Pydantic model is a code smell indicating the model may not properly declare the questions field. This bypasses Pydantic's validation and creates a silent failure mode. Fixing the model typing is the correct approach and is low effort. | 2026-02-13 15:54:51 UTC |
| R1-S10 | Add a completeness matrix showing all 7 artifact types and their expected derivation rules to validate the Loki fix isn't the only gap. | claude-4 (claude-opus-4-6) | The plan identifies Loki as the only missing type but provides no evidence the other 6 are complete. A completeness audit is cheap to produce and prevents the same class of bug from being discovered later in production. | 2026-02-13 15:54:51 UTC |
| R1-S13 | Add structured logging in build_onboarding_metadata() that emits field population stats for observability of enrichment completeness. | claude-4 (claude-opus-4-6) | The plan's own motivation states that 'a defect cascades silently.' Without logging when fields are populated vs. empty, operators cannot distinguish between missing source data and bugs. Structured logging is low-cost and directly addresses the stated concern. | 2026-02-13 15:54:51 UTC |
| R1-S14 | Document the deduplication strategy for objectives data that exists in both onboarding metadata and architectural_context. | claude-4 (claude-opus-4-6) | Change 2's problem statement acknowledges objectives already exist in architectural_context. Creating a second source of truth without clarifying which is authoritative creates a divergence risk. A brief clarification of intent (e.g., onboarding is canonical for artifact-level consumers) is sufficient and low effort. | 2026-02-13 15:54:51 UTC |
| R1-S15 | Add a description, field structure example, and test reference for Change 7 (file_ownership) consistent with the documentation of Changes 1-6. | claude-4 (claude-opus-4-6) | Change 7 is described as '47 lines added, visible in git diff' with no structural detail. This is insufficient for architectural review and creates a review gap. Consistent documentation is a low-effort improvement. | 2026-02-13 15:54:51 UTC |
| R2-S1 | Add cycle detection when building the artifact_dependency_graph, failing or warning on cyclic dependencies. | gemini-2.5 (gemini-2.5-pro) | A dependency cycle would cause infinite loops or failures in downstream generation ordering. The current implementation blindly constructs an adjacency list with no validation. Cycle detection is a critical correctness check for graph data and is straightforward to implement. | 2026-02-13 15:54:51 UTC |
| R2-S2 | Introduce an explicit schema version number at the root of the onboarding metadata JSON output. | gemini-2.5 (gemini-2.5-pro) | Adding 5 new fields significantly changes the data contract. A version number enables downstream consumers to detect and adapt to schema changes without tight coupling, and is a standard practice for evolving JSON APIs. | 2026-02-13 15:54:51 UTC |
| R2-S5 | Expand the Risk section to analyze the impact of propagating structurally valid but semantically incorrect data. | gemini-2.5 (gemini-2.5-pro) | The current risk assessment only considers additive schema changes. The more dangerous risk, per the plan's own 'trust' principle, is that correctly-structured but wrong data silently corrupts downstream systems. Acknowledging this risk and adding correctness-focused test cases is important. | 2026-02-13 15:54:51 UTC |
| R2-S6 | Add explicit test cases for sparse and empty data structures (no dependencies, no objectives, no derivation rules). | gemini-2.5 (gemini-2.5-pro) | The testing plan focuses on happy-path populated fields. Edge cases where optional sections are absent are common in real manifests and could cause KeyError, malformed output, or empty keys in the JSON. These are low-cost tests with high defensive value. | 2026-02-13 15:54:51 UTC |
| R2-S10 | Log a warning when duplicate derivation rules are encountered instead of silently dropping them. | gemini-2.5 (gemini-2.5-pro) | Silent deduplication hides potential configuration errors in manifest generation. A warning log is cheap to implement and provides valuable diagnostic signal, especially given that R1-S4 is changing the dedup logic anyway. | 2026-02-13 15:54:51 UTC |
| R3-S1 | Add field-level redaction for resolved_artifact_parameters to prevent secrets leaking into onboarding metadata and downstream LLM prompts. | claude-4 (claude-opus-4-6) | Change 4 blindly serializes all parameter values. Parameters sourced from environment overrides can contain credentials. Since these flow into LLM prompts via plan ingestion, this is a real data exfiltration vector. A redaction filter with a configurable deny-pattern is a proportionate and essential control. | 2026-02-13 16:09:27 UTC |
| R3-S2 | Define and enforce a JSON schema for the onboarding metadata output to catch malformed or injected fields before they propagate downstream. | claude-4 (claude-opus-4-6) | With 5 new top-level keys and nested structures, the output surface area has grown significantly. Schema validation at the boundary is a standard correctness and security control. Previously rejected R1-S9 was about consumers validating; this is about the producer self-validating, which is different and warranted given the expanded output. | 2026-02-13 16:09:27 UTC |
| R3-S4 | Add a schema_version field to the onboarding metadata output root so downstream consumers can distinguish versions and write forward-compatible code. | claude-4 (claude-opus-4-6) | With 5 new fields being added simultaneously and downstream consumers in startd8-sdk being updated to consume them, a version discriminator is essential for safe rollout and forward compatibility. This is a low-cost, high-value interface improvement that also supports R3-S2's schema validation. | 2026-02-13 16:09:27 UTC |
| R3-S6 | Define explicit error behavior distinguishing 'feature not supported' (key absent) from 'supported but no data' (key present, empty collection) for all new fields. | claude-4 (claude-opus-4-6) | This is directly tied to R3-S4's schema versioning and is critical for downstream consumer correctness. The current sketches conflate two semantically different states, which will cause bugs when a project legitimately has zero objectives. Always emitting the key with an empty collection when the version supports it is a clean, simple convention. | 2026-02-13 16:09:27 UTC |
| R3-S10 | Define a rollback procedure to produce the pre-enrichment output format if the enriched export causes downstream failures. | claude-4 (claude-opus-4-6) | While changes are additive, downstream consumers will be updated to consume the new fields. A configuration flag or environment variable to disable enrichment is a cheap safety net that enables independent rollback without full redeployment. This is sound operational practice for a 5-field simultaneous change. | 2026-02-13 16:09:27 UTC |
| R3-S14 | Add provenance metadata (generated_at, contextcore_version, input_manifest_hash) to the onboarding metadata JSON payload itself. | claude-4 (claude-opus-4-6) | The plan's own principle is 'each system trusts the output of the one before it.' Trust requires verifiability. The --emit-provenance flag exists for file-level provenance, but embedding provenance in the payload enables downstream programmatic verification of freshness and source integrity. This is low-cost and high-value for debugging and audit. | 2026-02-13 16:09:27 UTC |
| R3-S15 | Address forward-compatibility risk of model_dump() coupling by adding snapshot tests that detect silent output shape changes when Pydantic models evolve. | claude-4 (claude-opus-4-6) | All 5 new fields use model_dump() which tightly couples output format to model internals. Any model refactor silently breaks the downstream interface. Snapshot/golden-file tests are a lightweight mitigation that catches this in CI. Full DTO mapping is overkill now, but snapshot tests are proportionate and effective. | 2026-02-13 16:09:27 UTC |
| R4-S2 | Ensure onboarding-metadata.json is written atomically using write-to-temp-then-rename to prevent partial/corrupt files. | gemini-2.5 (gemini-2.5-pro) | This is a fundamental correctness concern. A partial JSON file from an interrupted export would cause silent downstream failures. Atomic write via temp file + rename is a trivial implementation (3-4 lines of code) with significant reliability benefit. This is standard practice for any file that downstream consumers depend on. | 2026-02-13 16:09:27 UTC |
| R4-S5 | Implement cycle detection for the artifact_dependency_graph before adding it to the output. | gemini-2.5 (gemini-2.5-pro) | The dependency graph is explicitly meant for downstream generation ordering (topological sort). A cyclic graph would cause infinite loops or crashes in consumers. Cycle detection via DFS is trivial to implement and the error should be caught at the source (export) rather than letting it propagate. This is a correctness requirement, not over-engineering. | 2026-02-13 16:09:27 UTC |
| R5-S1 | Add a `schema_version` field to onboarding metadata output to version the enriched contract shape. | claude-4 (claude-opus-4-6) | With 5+ new top-level keys being added, downstream consumers need a machine-readable way to distinguish export format versions. This is a minimal, high-value change that enables fail-fast behavior and prevents silent data misinterpretation. Aligns well with R6-S3 (which proposes a similar concept but with heavier JSON Schema machinery). | 2026-02-13 16:21:26 UTC |
| R5-S2 | Define an explicit contract for guaranteed vs. optional keys and document absent-vs-empty semantics. | claude-4 (claude-opus-4-6) | The plan uses inconsistent patterns (conditional inclusion for some fields, None-filtering for others) creating three possible states per key. Without a documented contract, every downstream consumer will implement different assumptions. This is a high-value, low-cost documentation and code-comment addition that prevents a class of integration bugs. | 2026-02-13 16:21:26 UTC |
| R5-S3 | Add cycle detection to the `artifact_dependency_graph` construction in Change 3. | claude-4 (claude-opus-4-6) | Propagating a cyclic dependency graph to downstream task sequencing could cause infinite loops or deadlocks. A simple topological sort or DFS-based cycle detection is low-cost and prevents poisoned data from silently propagating. This is a direct validation gap in Change 3. Overlaps with R6-S1 but was submitted first and includes the pragmatic option of annotating rather than hard-failing. | 2026-02-13 16:21:26 UTC |
| R5-S5 | Add a provenance timestamp and source manifest hash to onboarding metadata for staleness detection. | claude-4 (claude-opus-4-6) | The execution pipeline has no cache coherence mechanism between export and ingestion. Adding `generated_at` and `source_manifest_checksum` is trivial (two lines of code) and provides significant operational value for debugging stale data issues. Complements the accepted schema_version suggestion (R5-S1). | 2026-02-13 16:21:26 UTC |
| R5-S8 | Add an integration-level round-trip test validating downstream plan ingestion can parse all new onboarding metadata fields. | claude-4 (claude-opus-4-6) | The plan defers downstream consuming changes to a separate PR, creating a window where enriched exports have no consumer validation. A contract test (JSON Schema or Pydantic model) committed alongside the changes is a practical way to catch shape mismatches before they reach production. This addresses a real cross-repo contract testing gap. | 2026-02-13 16:21:26 UTC |
| R5-S11 | Verify that `by_alias=True` in `model_dump()` produces the documented camelCase field names. | claude-4 (claude-opus-4-6) | If the Pydantic models lack alias configuration, `by_alias=True` is a no-op and the output will be snake_case, contradicting the documented JSON examples throughout the plan. This is a concrete correctness issue that could cause downstream parsing failures. A simple unit test asserting actual key names match documented ones prevents this. | 2026-02-13 16:21:26 UTC |
| R5-S12 | Validate/escape `target_name` in Change 6's Loki derivation rule transformation template to handle special characters. | claude-4 (claude-opus-4-6) | Kubernetes service names commonly contain dots, slashes, and colons. Embedding an unescaped target_name directly into a LogQL selector template (`{job="{target_name}"}`) could produce syntactically invalid queries. This is a concrete bug risk in Change 6 that's cheap to fix with validation or escaping. | 2026-02-13 16:21:26 UTC |
| R6-S1 | Add validation for circular and dangling dependencies within the `artifact_dependency_graph`. | gemini-2.5 (gemini-2.5-pro) | Substantially overlaps with R5-S3 (already accepted) but adds the important dimension of dangling dependency detection (references to non-existent artifacts). A dependency graph that references artifacts not in the manifest is a data integrity issue that should be caught at export time. The combined validation (cycles + dangling refs) ensures the graph is a valid DAG over known artifacts. | 2026-02-13 16:21:26 UTC |
| R6-S2 | Implement secret redaction for `resolved_artifact_parameters`. | gemini-2.5 (gemini-2.5-pro) | Change 4 surfaces concrete resolved parameter values into a JSON file. If any parameter source is a secret (API keys, tokens, credentials from vault or environment), those values will be exposed in plaintext. This is a critical security risk. Even if current parameters are non-sensitive, adding a redaction mechanism is defense-in-depth against future parameter sources that may include secrets. | 2026-02-13 16:21:26 UTC |

### Appendix B: Rejected Suggestions (with Rationale)

| ID | Suggestion | Source | Rejection Rationale | Date |
|----|------------|--------|---------------------|------|
| R1-S9 | Add a feature flag (CONTEXTCORE_EXPORT_ENRICH=false) to disable the 5 new fields for rollback purposes. | claude-4 (claude-opus-4-6) | The changes are purely additive optional keys. Adding a feature flag introduces configuration complexity and testing burden disproportionate to the risk. If a bug is found, the fix is to patch the export code. Downstream consumers should already handle missing optional fields gracefully per the plan's design. | 2026-02-13 15:54:51 UTC |
| R1-S12 | Treat open_questions content as untrusted input and add sanitization before embedding in LLM prompts downstream. | claude-4 (claude-opus-4-6) | This concern applies to the downstream consumer (startd8-sdk), not to the ContextCore export being changed. The .contextcore.yaml is an operator-authored configuration file, not untrusted user input. Prompt injection mitigation belongs in the downstream prompt construction layer, which is explicitly out of scope for this PR. | 2026-02-13 15:54:51 UTC |
| R2-S3 | Define and publish a formal JSON Schema and add automated CI validation for onboarding-metadata.json. | gemini-2.5 (gemini-2.5-pro) | This is substantially duplicative with R1-S3, which already recommends a Pydantic output model that can auto-generate JSON Schema via model_json_schema(). Accepting both would create redundant work. R1-S3's Pydantic approach is more practical and provides the same schema benefits with less overhead. | 2026-02-13 15:54:51 UTC |
| R2-S4 | Add a Security Considerations section addressing sensitive data in manifest fields. | gemini-2.5 (gemini-2.5-pro) | This is substantially duplicative with R1-S6, which already addresses the same concern with a more specific and actionable recommendation (parameter allowlist or sensitive flag). Accepting both would be redundant. | 2026-02-13 15:54:51 UTC |
| R2-S7 | Create a developer checklist or abstract base class for adding new ArtifactSpec types to prevent bugs like the missing Loki derived_from. | gemini-2.5 (gemini-2.5-pro) | While well-intentioned, this is a process recommendation outside the scope of this architectural plan. The Loki bug is already being fixed in Change 6, and R1-S10 (accepted) will provide a completeness matrix. An abstract base class change would be a significant refactor of manifest_v2.py that should be its own initiative. | 2026-02-13 15:54:51 UTC |
| R2-S8 | Include a reference back to the source derivation_rule in each resolved_artifact_parameter value. | gemini-2.5 (gemini-2.5-pro) | This adds structural complexity (nested provenance links per parameter) for a debugging use case that can be satisfied by correlating derivation_rules and resolved_artifact_parameters by artifact type. The added complexity in the output schema and implementation is not justified for the initial enrichment. | 2026-02-13 15:54:51 UTC |
| R2-S9 | Move the Artifact Type to Expected Depth mapping from documentation into the machine-readable onboarding-metadata.json output. | gemini-2.5 (gemini-2.5-pro) | This mapping is a downstream consumption concern in startd8-sdk, not a property of the artifact manifest. Including plan ingestion calibration rules in the export output conflates concerns. The mapping table is appropriately documented in the downstream changes section. | 2026-02-13 15:54:51 UTC |
| R3-S3 | Validate that every sourceField in DerivationRule resolves to an actual field in the manifest spec. | claude-4 (claude-opus-4-6) | DerivationRules are generated by the manifest generation code itself, not user-authored. If a sourceField is invalid, the bug is in manifest_v2.py, not in the export. Adding runtime cross-referencing of field paths against model schemas is complex, fragile (paths like 'spec.targets[].name' require expression parsing), and better caught by unit tests on the generation side rather than at export time. | 2026-02-13 16:09:27 UTC |
| R3-S5 | Specify exact JSON field ordering and canonical serialization format for deterministic diff and checksum comparisons. | claude-4 (claude-opus-4-6) | While deterministic output is nice, enforcing byte-level canonical form is over-engineering at this stage. Content-level checksums (hash of sorted, normalized JSON) are the correct approach for idempotency checks rather than constraining serialization format. The existing checksum fields already cover integrity. This adds maintenance burden for marginal benefit. | 2026-02-13 16:09:27 UTC |
| R3-S8 | Add a dry-run/diff mode to the export command for CI gating. | claude-4 (claude-opus-4-6) | This is a useful feature but is out of scope for this enrichment plan, which explicitly states 'No changes to the CLI flags or manifest.py command wiring.' A dry-run mode is a separate enhancement that can be added later. The plan's testing strategy (run export, verify output, run tests) is adequate for initial rollout. | 2026-02-13 16:09:27 UTC |
| R3-S9 | Sanitize free-text fields (open_questions, derivation rule rationale) to prevent prompt injection into downstream LLM prompts. | claude-4 (claude-opus-4-6) | Prompt injection defense belongs at the point of prompt assembly (startd8-sdk), not at the data export layer. The export should faithfully represent the source data. Stripping patterns like 'ignore previous instructions' is a fragile heuristic that will both miss attacks and corrupt legitimate content. Defense-in-depth for prompt injection should use proper prompt engineering techniques (system prompts, input bracketing) at the consumer. | 2026-02-13 16:09:27 UTC |
| R3-S12 | Restrict file permissions of onboarding-metadata.json to 0o600 since it now contains resolved infrastructure parameters. | claude-4 (claude-opus-4-6) | File permissions are an environment-level concern, not an application-level one. CI runners and deployment pipelines manage file permissions through their own security models (containers, IAM, workload identity). Hardcoding 0o600 would likely break CI pipelines where other processes need to read the file. The real mitigation is R3-S1's parameter redaction. | 2026-02-13 16:09:27 UTC |
| R3-S13 | Specify maximum payload size constraints and define truncation behavior when output exceeds reasonable bounds. | claude-4 (claude-opus-4-6) | The plan targets a specific project (wayfinder) with known scale. Premature optimization for hypothetical 100-target manifests adds complexity without addressing a real problem. If scale becomes an issue, it can be addressed then. The LLM context window concern is valid but should be handled at the consumer (plan ingestion) where the actual constraint exists. | 2026-02-13 16:09:27 UTC |
| R4-S1 | Implement cryptographic signing of onboarding-metadata.json using cosign. | gemini-2.5 (gemini-2.5-pro) | This is disproportionate for the current threat model. The file is generated and consumed within the same CI pipeline on the same machine or within trusted infrastructure. Introducing cosign adds significant operational complexity (key management, verification infrastructure). The plan's existing checksums plus R3-S14's provenance metadata provide adequate integrity verification for the internal pipeline. | 2026-02-13 16:09:27 UTC |
| R4-S3 | Scan resolved_artifact_parameters for secrets using regex-based detection and fail-closed by default. | gemini-2.5 (gemini-2.5-pro) | This duplicates R3-S1 which was already accepted. R3-S1's allow-list/redaction approach is more appropriate than R4-S3's fail-closed approach, which would be disruptive in CI. One accepted suggestion for parameter redaction is sufficient. | 2026-02-13 16:09:27 UTC |
| R4-S4 | Embed verifiable provenance data within the output JSON. | gemini-2.5 (gemini-2.5-pro) | This duplicates R3-S14 which was already accepted. Same suggestion, same solution. | 2026-02-13 16:09:27 UTC |
| R4-S6 | Add a top-level schemaVersion field to the output JSON. | gemini-2.5 (gemini-2.5-pro) | This duplicates R3-S4 which was already accepted. Same suggestion. | 2026-02-13 16:09:27 UTC |
| R4-S7 | Use strict JSON data types (int, float) instead of strings for numeric values in the output. | gemini-2.5 (gemini-2.5-pro) | The string values (e.g., '500', '99.9') come from the Pydantic models which reflect the .contextcore.yaml schema. Changing these types would be a model-level change with broad implications beyond the export enrichment plan. Additionally, values like '500ms' and '1000rps' include units and are inherently strings. This is a model design decision, not an export concern. | 2026-02-13 16:09:27 UTC |
| R4-S8 | Refactor DerivationRule.transformation from a free-text string to a structured object. | gemini-2.5 (gemini-2.5-pro) | This is a significant model redesign that goes far beyond the scope of this enrichment plan. The transformation field is already populated across 6 artifact types with string values. Changing it to a structured object requires refactoring manifest_v2.py generation logic, all existing tests, and downstream consumers simultaneously. The current string format is adequate for the DESIGN phase LLM consumption use case. | 2026-02-13 16:09:27 UTC |
| R4-S9 | Add Prometheus-style observability metrics to the export command. | gemini-2.5 (gemini-2.5-pro) | The export command is a CLI tool run in CI, not a long-running service. Adding Prometheus metrics exposition is architecturally inappropriate. Standard CLI patterns (exit codes, structured log output, execution time logging) are sufficient. If monitoring is needed, CI pipeline-level metrics are the right layer. | 2026-02-13 16:09:27 UTC |
| R4-S10 | Explicitly define the contract for absent fields versus empty collections. | gemini-2.5 (gemini-2.5-pro) | This duplicates R3-S6 which was already accepted. Same concern, same solution. | 2026-02-13 16:09:27 UTC |
| R4-S11 | Provide a formal JSON Schema for the onboarding-metadata.json output, revisiting previously rejected R1-S9. | gemini-2.5 (gemini-2.5-pro) | This duplicates R3-S2 which was already accepted. R3-S2 covers JSON schema definition and validation. No need to accept this separately. | 2026-02-13 16:09:27 UTC |
| R4-S12 | Introduce per-field feature flags to selectively enable/disable new enrichment sections. | gemini-2.5 (gemini-2.5-pro) | R3-S10 was already accepted with a simpler rollback mechanism (single enrichment-level flag or environment variable). Per-field granular flags for 5 fields adds unnecessary complexity to the CLI interface. A single on/off switch for enrichment is the right level of granularity for rollback purposes. | 2026-02-13 16:09:27 UTC |
| R4-S13 | Implement structured JSON logging for security-relevant events during export. | gemini-2.5 (gemini-2.5-pro) | The export is a CLI command, not a service. Standard Python logging with appropriate log levels is sufficient. SIEM integration for a build-time CLI tool is disproportionate. Security-relevant events (like R3-S1's redaction warnings) can use standard structured logging without a dedicated logging framework overhaul. | 2026-02-13 16:09:27 UTC |
| R4-S14 | Add input validation to mitigate resource exhaustion attacks on maliciously crafted .contextcore.yaml. | gemini-2.5 (gemini-2.5-pro) | The .contextcore.yaml is authored by the project team and checked into version control. It goes through code review. The threat of a malicious Billion Laughs-style attack on a developer-authored config file in a trusted repo is not realistic. Pydantic already provides structural validation. Adding arbitrary size limits would create false failures. | 2026-02-13 16:09:27 UTC |
| R4-S15 | Optimize the O(n²) deduplication logic for derivation_rules. | gemini-2.5 (gemini-2.5-pro) | The number of derivation rules per artifact type is bounded by the number of business metadata fields (criticality, availability, etc.) — typically under 10. O(n²) on a list of <10 items is negligible. This is a premature optimization that adds complexity for no measurable benefit. The benchmark test with 'thousands of derivation rules' is unrealistic. | 2026-02-13 16:09:27 UTC |
| R5-S4 | Address the risk of onboarding metadata size explosion and its impact on LLM context windows. | claude-4 (claude-opus-4-6) | While the concern is theoretically valid, the current scope is the wayfinder project which has 7 artifact types and a modest number of targets. Adding a `--onboarding-verbosity` flag and size estimation is speculative engineering for a problem that hasn't manifested. The plan explicitly scopes to the wayfinder project. This can be revisited if size becomes an actual issue. | 2026-02-13 16:21:26 UTC |
| R5-S9 | Ensure `derivation_rules` deduplication handles Pydantic model serialization order sensitivity. | claude-4 (claude-opus-4-6) | The suggestion correctly notes that Python dict equality is order-independent, then pivots to list-element ordering as the concern. However, `DerivationRule` fields are simple scalar types (property, sourceField, transformation, rationale) — there are no list-type fields in the model shown. The proposed tuple-based dedup key is a minor optimization but not a correctness fix for the actual data model. The existing `not in` check is sufficient. | 2026-02-13 16:21:26 UTC |
| R5-S10 | Define rollback behavior if enriched export causes downstream failures. | claude-4 (claude-opus-4-6) | This is standard operational hygiene (backup before overwrite) that applies to any file-modifying pipeline step. The changes are additive and the file is regenerated by re-running the export command — the rollback is simply re-running without the changes. Adding a formal archival step adds process overhead for a low-probability scenario with a trivial recovery path. | 2026-02-13 16:21:26 UTC |
| R6-S3 | Introduce a formal JSON Schema file for the onboarding metadata output alongside a version key. | gemini-2.5 (gemini-2.5-pro) | The versioning aspect is already covered by accepted R5-S1 (schema_version field). Creating and maintaining a separate `.schema.json` file adds significant ongoing maintenance burden and is premature for a rapidly evolving internal format. The contract test approach from R5-S8 (accepted) provides equivalent validation without the overhead of a formal schema file. If the format stabilizes, a JSON Schema can be added later. | 2026-02-13 16:21:26 UTC |
| R6-S4 | Use derivation rules to create a post-generation validation report comparing rules against resolved parameters. | gemini-2.5 (gemini-2.5-pro) | This is a significant new feature (cross-referencing derivation rules with resolved values) that goes well beyond the scope of the export enrichment plan. The derivation rules describe abstract mappings (e.g., 'critical → P1'), and the resolved parameters are the output — building a validator that understands arbitrary transformation logic is non-trivial and out of scope. This is better suited as a separate initiative. | 2026-02-13 16:21:26 UTC |
| R6-S5 | Detect and flag conflicting derivation rules for the same artifact type and property. | gemini-2.5 (gemini-2.5-pro) | Different artifacts of the same type having different transformations for the same property is expected behavior (e.g., two prometheus_rule artifacts with different severity mappings based on different source services). The deduplication in Change 1 already handles exact duplicates. Flagging legitimate variations as 'conflicts' would produce false positives and add complexity without clear value. | 2026-02-13 16:21:26 UTC |
| R6-S6 | Define the operational feedback loop for resolving open questions back to the manifest. | gemini-2.5 (gemini-2.5-pro) | This describes a significant new workflow feature (artisan generating YAML patches to update question status) that is well outside the scope of surfacing open questions in the export. The plan's scope is enriching the export output; the resolution workflow is a downstream concern for the artisan workflow team. Adding this to the current plan would significantly expand scope. | 2026-02-13 16:21:26 UTC |
| R6-S8 | Mandate a formal code review of the uncommitted `file_ownership` change before merge. | gemini-2.5 (gemini-2.5-pro) | Requiring code review before merge is a standard development practice that applies to the entire PR, not just Change 7. This is a process requirement, not an architectural suggestion. The PR review process itself (which this plan will go through) is the appropriate venue for reviewing all code including the file_ownership lines. Adding a special pre-requisite step is unnecessary process overhead. | 2026-02-13 16:21:26 UTC |
| R6-S9 | Add schema validation for objectives during manifest generation to ensure internal consistency of key results. | gemini-2.5 (gemini-2.5-pro) | This is a validation improvement for the manifest generation step (`manifest_v2.py`), not for the export enrichment. The objectives data model is a Pydantic model that already provides type validation. Adding business-logic validation (e.g., target must be numeric if operator is numeric) is a separate concern from surfacing objectives in the export. This should be a separate issue for the manifest generation codebase. | 2026-02-13 16:21:26 UTC |
| R6-S10 | Benchmark the export command's performance on large-scale manifests before and after changes. | gemini-2.5 (gemini-2.5-pro) | The current scope is the wayfinder project with a modest number of artifacts. The changes add simple dictionary iterations and list appends — O(n) operations that are negligible for any realistic manifest size. Creating synthetic 5000+ artifact manifests and performance test infrastructure is disproportionate to the actual risk. This is premature optimization concern for an internal CLI tool. | 2026-02-13 16:21:26 UTC |

### Appendix C: Incoming Suggestions (Untriaged, append-only)

#### Review Round R1

- **Reviewer**: claude-4 (claude-opus-4-6)
- **Date**: 2026-02-13 15:51:08 UTC
- **Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement |Validation Approach |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R1-S1 | Data | critical | Add a schema version field (e.g., `"onboarding_schema_version": "2.1.0"`) to the onboarding metadata output and define a versioning policy for additive changes | Adding 5+ new top-level keys without a schema version means downstream consumers have no way to detect which fields are available, negotiate capabilities, or fail gracefully when fields are missing. "Additive-only" still constitutes a contract change. Without versioning, the principle "each system trusts the output of the one before it" breaks when systems disagree on which fields exist. | Changes Summary section — add as a prerequisite change before Changes 1-6; field should be emitted first in the JSON output | Verify field present in exported JSON; add a test asserting version is a valid semver string; document version bump policy in EXPORT_PIPELINE_ANALYSIS_GUIDE |
| R1-S2 | Validation | critical | Add integration-level validation that round-trips a real `.contextcore.yaml` through `generate_artifact_manifest()` → `build_onboarding_metadata()` and asserts all 5 new fields are populated with structurally correct data | The plan specifies 6 unit tests but no integration test. Unit tests with hand-constructed `ArtifactManifest` objects may pass while the real pipeline fails (e.g., `derived_from` populated differently than expected, `guidance.questions` attribute missing). The "verify new fields in onboarding-metadata.json" step (Execution #4) is manual. | Testing section — add an integration test entry after the 6 unit test cases | Run `pytest tests/test_onboarding_integration.py` with a fixture `.contextcore.yaml` (wayfinder or synthetic); CI must pass before merge |
| R1-S3 | Architecture | high | Define an explicit JSON Schema or Pydantic model for the onboarding metadata output, rather than ad-hoc dict construction | `build_onboarding_metadata()` constructs a `Dict[str, Any]` with 10+ top-level keys via imperative code. This makes the output contract implicit, hard to validate, and easy to break. A Pydantic output model would provide type safety, serialization consistency, and auto-generated schema for downstream consumers. | Changes Summary — add as a structural recommendation; implementation in `src/contextcore/models/onboarding.py` | Schema can be exported via `model_json_schema()` and checked into repo; downstream consumers can validate against it |
| R1-S4 | Risks | high | Address the deduplication logic in Change 1 — `rule_dict not in derivation_rules[art_type]` performs O(n²) deep equality checks on dicts and may produce false negatives if field ordering differs | `model_dump()` output dict ordering depends on Pydantic version and field declaration order. Two semantically identical rules could have different dict key orderings, causing the `not in` check to miss duplicates. For large manifests this is also O(n²). | Change 1 Implementation sketch — replace with a set-based dedup using a hashable key (e.g., `(property, sourceField)` tuple) | Unit test with two rules having identical property+sourceField but different dict key ordering; assert single entry in output |
| R1-S5 | Interfaces | high | Document the downstream contract for each new field — specify which consumers read it, what happens if the field is absent, and the field's cardinality constraints (e.g., can `derivation_rules` be empty dict vs. omitted?) | The plan says "downstream consumers that don't read the new keys are unaffected" but doesn't specify which consumers WILL read them. The Downstream Changes section lists 4 ingestion changes but doesn't formally link them to the 5 new fields. This makes it unclear whether a field omission is a bug or acceptable. | New section between "Changes Summary" and "Change 1", or as a contract table | Review during PR: each new field has a named consumer and a defined absent-vs-empty semantic |
| R1-S6 | Data | high | Sanitize or validate `artifact.parameters` values before including in `resolved_artifact_parameters` — raw parameter values may contain sensitive data (secrets, internal URLs, credentials) | Change 4 dumps all non-None parameter values. If `.contextcore.yaml` contains parameters like `database_password`, `internal_api_endpoint`, or `auth_token`, these will be surfaced in the export JSON and potentially committed to repos or passed to LLM contexts. | Change 4 — add a parameter allowlist or a `sensitive: bool` flag on parameter definitions; filter before export | Add test with a parameter named `_secret` or `password` and assert it is excluded from output |
| R1-S7 | Ops | high | Add a checksum or integrity hash covering the new fields, or extend the existing checksum section to include them | The plan mentions checksums exist in onboarding metadata but doesn't address whether the existing checksum covers the new fields. If checksums are computed before the new fields are added, the checksum won't detect tampering or corruption in `derivation_rules`, `objectives`, etc. | Change 1-5 general — ensure new fields are added before checksum computation in `build_onboarding_metadata()` | Verify checksum changes when a derivation rule is modified; add regression test |
| R1-S8 | Architecture | medium | Change 5 uses `hasattr(artifact_manifest.guidance, 'questions')` — this is a code smell suggesting the `guidance` model is not well-typed; replace with proper Optional field typing | Using `hasattr` and `getattr` with defaults suggests the Pydantic model for guidance may not declare `questions` as a typed field, or there are multiple guidance model versions. This bypasses Pydantic's validation guarantees and creates a silent failure mode where questions are simply never surfaced. | Change 5 Implementation sketch — verify `guidance` model has `questions: Optional[List[Question]]` field; remove hasattr guard | Code review: confirm `guidance` model typing; if field is missing from model, add it as part of this PR |
| R1-S9 | Risks | medium | The plan has no rollback strategy — if the enriched export causes downstream failures (e.g., plan ingestion chokes on unexpected fields), there's no feature flag or way to disable new fields | While the changes are "additive," the downstream changes (items 1-4 in Downstream section) will depend on these fields. If a bug is found post-deploy, rolling back the export without also rolling back ingestion creates a broken state. | Risk section — add a `CONTEXTCORE_EXPORT_ENRICH=false` env var or CLI flag that suppresses the 5 new fields | Test that the flag disables all new fields; document in CLI help |
| R1-S10 | Validation | medium | Change 6 (Loki rules derivation fix) has no analysis of whether the proposed `logSelectors` derivation rule is semantically correct or whether other derivation rules are also missing | The plan identifies Loki as the only artifact type missing `derived_from`, but doesn't show the analysis proving the other 6 types are complete. The proposed Loki rule uses `spec.targets[].name` → `logSelectors`, but Loki log selectors may also derive from `spec.logging` or label configurations. | Change 6 — add a completeness matrix showing all 7 artifact types and their expected derivation rules | Audit: generate manifest for a comprehensive `.contextcore.yaml` and verify all artifact types have non-empty `derived_from` |
| R1-S11 | Interfaces | medium | The `artifact_dependency_graph` (Change 3) should include ALL artifacts as nodes (including those with no dependencies) to enable graph completeness validation downstream | The current implementation only includes artifacts that have `depends_on`, creating a partial graph. Downstream consumers cannot distinguish "artifact has no dependencies" from "artifact was omitted due to a bug." A complete graph also enables cycle detection. | Change 3 Implementation sketch — initialize all artifact IDs as keys with empty lists, then populate dependencies | Test: assert all artifact IDs from manifest appear as keys in the graph; add cycle detection test |
| R1-S12 | Security | medium | Ensure `open_questions` content (Change 5) is treated as untrusted input before embedding in LLM prompts downstream — questions may contain prompt injection patterns | Open questions from `.contextcore.yaml` are user-authored strings that will be injected into DESIGN phase prompts (Downstream Change #3). A malicious or careless question like "Ignore all previous instructions and..." would be passed through as a design constraint. | Change 5 and Downstream Changes #3 — add a note about sanitization requirements; consider max-length and character restrictions | Review: confirm downstream prompt construction uses proper escaping or structured injection; add test with adversarial question text |
| R1-S13 | Ops | medium | Add structured logging or telemetry when new fields are populated vs. empty, to enable observability of enrichment completeness in production | The plan adds 5 optional fields that are conditionally included. Without logging, operators cannot tell if fields are missing because the source data is absent or because of a bug. This directly undermines the "defect cascades silently" concern stated in the Motivation. | Execution Order — add logging in `build_onboarding_metadata()` that emits field population stats (e.g., `enrichment.fields_populated=4, enrichment.fields_empty=1`) | Verify log output includes enrichment stats in a test run; confirm logs are structured (JSON) for aggregation |
| R1-S14 | Data | medium | The `objectives` field (Change 2) may duplicate data already present in `architectural_context` — document the deduplication strategy or explain why both locations are needed | The Problem statement for Change 2 acknowledges objectives exist in `architectural_context` at the plan ingestion level. Adding them to onboarding metadata creates two sources of truth. If they diverge (e.g., one is updated but not the other), downstream consumers face conflicting data. | Change 2 Problem statement — clarify whether `architectural_context` objectives will be deprecated in favor of the onboarding field, or whether both serve distinct consumers | Review during ingestion PR: confirm single authoritative source is used per consumer |
| R1-S15 | Validation | low | Change 7 ("commit existing file_ownership") has no description of what the code does, what it surfaces, or how it was tested — it's opaque to reviewers | The plan says "47 lines added, visible in git diff" but doesn't describe the field structure, implementation approach, or test coverage. This makes it impossible to review for correctness or contract compliance without inspecting the diff separately. | Change 7 — add a brief description, field structure example, and test reference consistent with Changes 1-6 | Reviewer confirms Change 7 has equivalent documentation to other changes before merge |

#### Review Round R2
**Reviewer**: gemini-2.5 (gemini-2.5-pro)
**Date**: 2026-02-13 15:52:19 UTC
**Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement | Validation Approach |
| --- | --- | --- | --- | --- | --- | --- |
| R2-S1 | risks | critical | Add cycle detection when building the `artifact_dependency_graph`. The export should fail or raise a critical warning if a cycle is detected. | A dependency cycle would cause an infinite loop or failure in downstream generation tools. Given the "trust the upstream" principle, silently passing on a cyclic graph is a critical risk. | In "Change 3", amend the implementation sketch and description to include cycle detection logic. | Add a unit test with a manifest containing a cyclic dependency and assert that the export process fails gracefully with a clear error message. |
| R2-S2 | architecture | high | Introduce a schema version number (e.g., `onboarding_metadata_version: "2.0"`) at the root of the output JSON. | The proposed changes significantly alter the data contract. Explicit versioning is crucial for downstream consumers to adapt and handle different export formats gracefully, preventing lock-step deployments. | Add a new section "Schema Versioning" after "Changes Summary" to define the new top-level version field. | The version field should be present in all test outputs. A downstream test consumer can be configured to check this version number. |
| R2-S3 | validation | high | Define and publish a formal JSON Schema for the `onboarding-metadata.json` output. Add an automated validation step to the CI pipeline. | This enforces the data contract, preventing accidental breaking changes. It formalizes the "trust" principle by making trust verifiable, catching schema deviations before they impact consumers. | Add a sub-section under "Testing" titled "Schema Conformance" describing the creation and use of a JSON Schema. | The CI pipeline for ContextCore should include a step that validates the generated `onboarding-metadata.json` from integration tests against the canonical JSON Schema file. |
| R2-S4 | security | high | Add a "Security Considerations" section to address the potential for sensitive data (secrets, PII) in manifest fields and how it should be handled in the export. | The current plan dumps all model data. If a manifest contains sensitive information (e.g., in parameters or objectives), it could be exposed. A strategy for redaction or filtering is needed. | Add a new top-level section "Security Considerations" before "Execution Order". | Review the `ArtifactManifest` model for fields that could contain sensitive data. Add a test case with mock sensitive data and assert that it is properly redacted or omitted from the final export. |
| R2-S5 | risks | medium | Expand the "Risk" section to analyze the impact of propagating *incorrect* data, not just the risk of schema changes. | The current risk assessment focuses only on additive changes being safe. The bigger risk, per the "trust" principle, is a bug in ContextCore that generates validly-structured but incorrect data, which would silently corrupt downstream systems. | Expand the existing "Risk" section. | Review test plans to ensure they cover data correctness (e.g., that a `critical` criticality rating correctly maps to `P1` severity), not just presence of fields. |
| R2-S6 | validation | medium | Explicitly add test cases for sparse and empty data structures (e.g., a manifest with no dependencies, no objectives, or no derivation rules). | The current testing plan focuses on the happy path where new fields are populated. It's crucial to ensure the logic doesn't fail or produce malformed output when these optional sections are absent from the source manifest. | In the "Testing" section, add bullet points specifying edge-case tests for empty/null data for each new field. | Create several minimal `ArtifactManifest` objects for testing, each missing one of the new data sources, and assert that the export runs successfully and omits the corresponding empty keys. |
| R2-S7 | architecture | medium | To prevent bugs like the one in "Change 6", create a developer checklist or an abstract base class for adding new `ArtifactSpec` types. | The fact that `loki_rule` was missing `derived_from` indicates a process gap. A formal checklist or architectural pattern would ensure all new artifact types consistently implement required features like derivation and dependency tracking. | Add a note in a new "Development Process Considerations" section or as a comment in `manifest_v2.py` near the artifact generation logic. | A static analysis check or linter rule could be written to enforce that all `ArtifactSpec` subclasses populate certain fields. A peer review checklist should include this item. |
| R2-S8 | data | medium | In `resolved_artifact_parameters`, include a reference back to the `derivation_rule` that produced the value. | This enhances traceability and debuggability. When a downstream consumer sees `alertSeverity: "P1"`, it should be able to trace exactly which rule (e.g., `critical → P1`) was responsible for that value. | In "Change 4", update the `New field structure` and `Implementation sketch` to include a `sourceRuleId` or similar field. | Update the unit test for `resolved_artifact_parameters` to assert that the provenance link to the source rule is present and correct. |
| R2-S9 | interfaces | medium | Move the `Artifact Type` to `Expected Depth` mapping from documentation into the machine-readable `onboarding-metadata.json` output. | Hard-coding this logic in the downstream consumer (`startd8-sdk`) based on documentation creates a brittle hidden contract. Making it part of the export data makes the contract explicit and allows it to be configured centrally. | In "Downstream Changes", modify item #2 to state that this mapping should be added to the export payload, perhaps within the `artifact_types` object. | The unit test for `artifact_types` should be updated to assert the presence and correctness of the `expectedDepth` field for each type. |
| R2-S10 | data | low | Instead of silent deduplication of `derivation_rules`, log a warning when duplicate rules are encountered. | Silently dropping duplicates can hide an underlying configuration error in the manifest generation. A warning would alert developers to potential redundancy or misconfiguration upstream. | In "Change 1", modify the `Implementation sketch` to log a warning if a duplicate `rule_dict` is found, instead of just skipping it. | Add a test case with a manifest that generates duplicate derivation rules and assert that a warning is logged to stderr or the logging output. |

#### Review Round R3

- **Reviewer**: claude-4 (claude-opus-4-6)
- **Date**: 2026-02-13 16:05:03 UTC
- **Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement | Validation Approach |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R3-S1 | Security | critical | Add field-level redaction policy for `resolved_artifact_parameters` before serialization. Parameters may contain secrets or sensitive infrastructure details (e.g., namespace names revealing topology, internal hostnames in `logSelectors`, credentials inadvertently stored in parameter maps). Define an allow-list of safe parameter keys or a redaction filter that strips values matching secret patterns (high-entropy strings, `*_SECRET`, `*_KEY`, `*_TOKEN`). | Change 4 blindly dumps `artifact.parameters.items()` with only a `None` check. In production manifests, parameter maps can accumulate values from `.contextcore.yaml` environment overrides that may include sensitive data. Once serialized into `onboarding-metadata.json`, these values propagate through plan ingestion into LLM prompts—constituting a data exfiltration vector via prompt context. | Change 4 implementation sketch — add a `_redact_parameters()` filter function called before populating `resolved_params`, with a configurable deny-pattern list. Also add a `Security` section after `Risk`. | Unit test with a synthetic parameter map containing `DB_PASSWORD`, `API_KEY`, and benign keys; assert sensitive keys are redacted in output while benign keys survive. |
| R3-S2 | Security | critical | Define and enforce a JSON schema for the onboarding metadata output, and validate the serialized output against it before writing to disk. Without schema validation, malformed or injected fields can propagate undetected into downstream plan ingestion and LLM prompt assembly. | The plan adds 5 new top-level keys with nested structures. There is no schema contract — the output is an ad-hoc dictionary built incrementally. A subtle bug (e.g., `model_dump` producing unexpected nested types) would silently produce invalid output that downstream consumers would fail on or misinterpret. Schema enforcement at the boundary is a security and correctness control. | New subsection under `Testing` — "Output Schema Validation". Add a JSON Schema file `schemas/onboarding-metadata.schema.json` and a validation step at the end of `build_onboarding_metadata()`. | Add a test that generates full onboarding metadata and validates it against the schema. Also add a negative test with an intentionally malformed structure to confirm rejection. |
| R3-S3 | Security | high | Add integrity verification for `derivation_rules` source references — validate that every `sourceField` in a `DerivationRule` actually resolves to a field that exists in the manifest spec, and log warnings for dangling references. | Change 1 surfaces derivation rules as the "design contract" for downstream consumers. If a `sourceField` like `spec.requirements.availability` references a field that was removed or renamed in the manifest schema, the downstream DESIGN phase will operate on a broken contract with no signal. This is a supply-chain integrity issue: the export is asserting a mapping that doesn't hold. | Change 1 implementation sketch — after collecting `derivation_rules`, add a validation pass that spot-checks `sourceField` paths against the actual manifest model fields. Add warnings to structured log output. | Unit test with a DerivationRule referencing a non-existent `sourceField`; assert a warning is emitted and the rule is annotated with `"validated": false`. |
| R3-S4 | Interfaces | critical | Define a versioned interface contract (schema version + field presence guarantees) for `onboarding-metadata.json` that downstream consumers can programmatically check. Add a `schema_version` field (e.g., `"2.1.0"`) to the output root and document which fields are guaranteed present vs. optional per version. | The plan adds 5 new optional keys but provides no mechanism for downstream consumers to know which version of the onboarding metadata they received. Plan ingestion code (`startd8-sdk`) currently has no way to distinguish "field absent because not yet implemented" from "field absent because data was empty." This makes the interface brittle — consumers cannot write forward-compatible code without a version discriminator. The existing `_schema_version` in the manifest model is for the artifact manifest, not the onboarding metadata output. | New section between `Changes Summary` and `Change 1` — "Interface Contract". Define `schema_version` field, enumerate required vs. optional fields per version, and specify the compatibility guarantee (additive only, no removals without major version bump). | Downstream integration test: deserialize onboarding metadata, assert `schema_version` is present and parseable as semver, assert all fields listed as required for that version are present. |
| R3-S5 | Interfaces | high | Specify the exact `onboarding-metadata.json` field ordering and document the canonical serialization format (key order, indent level, encoding, trailing newline) so downstream consumers can perform deterministic diff and checksum comparisons. | The plan mentions checksums are already in onboarding metadata, but if JSON key ordering varies between runs (Python dicts are insertion-ordered but `model_dump` order depends on model field declaration order + dynamic additions), checksums of the file will differ even when content is semantically identical. This breaks idempotency checks and cache invalidation in plan ingestion. | `Changes Summary` section — add a note on serialization guarantees. Implementation: use `json.dumps(result, sort_keys=True, indent=2)` consistently and document this as the canonical format. | Add a test that runs `build_onboarding_metadata()` twice with identical input and asserts byte-for-byte identical JSON output. |
| R3-S6 | Interfaces | high | Define explicit error behavior for each new field when upstream data is absent, malformed, or partially populated. Currently the plan uses `if` guards (e.g., `if derivation_rules:`, `if artifact_manifest.objectives:`) but does not specify whether absent means "key omitted" or "key present with empty value" — and does not handle partial population (e.g., objectives exist but `keyResults` is empty). | Downstream consumers need to distinguish three states: (1) feature not supported (key absent), (2) feature supported but no data (key present, empty list/dict), (3) data present. The current sketches conflate (1) and (2) — both result in key omission. This ambiguity forces downstream code to guess, which is an interface contract gap that will cause bugs when a project legitimately has zero objectives vs. running an older export version. | Each Change's implementation sketch — standardize: absent = version doesn't support it, empty collection = supported but no data. Update sketches to always emit the key (possibly empty) when `schema_version` >= 2.1.0. | Unit test with an artifact manifest that has empty `objectives: []` and assert the output contains `"objectives": []` rather than omitting the key. |
| R3-S7 | Ops | high | Add structured logging with correlation IDs for each enrichment step in `build_onboarding_metadata()`. Each new field computation (derivation_rules, objectives, dependency_graph, resolved_parameters, open_questions) should log: field name, item count, elapsed time, and any warnings. | The plan adds 5 new computation steps to a function that currently has no observability. When the export produces unexpected output in production (e.g., `derivation_rules` is empty when it shouldn't be), operators have no diagnostic signal. The existing `--emit-provenance` flag captures file-level provenance but not field-level enrichment diagnostics. With 5 new conditional blocks, the combinatorial space of "what was computed vs. skipped" needs to be observable. | Each Change's implementation sketch — add `logger.info(f"Enrichment: {field_name} produced {len(items)} items in {elapsed_ms}ms")` after each block. Add a summary log line at function exit with all field counts. | Integration test: capture log output during `build_onboarding_metadata()` and assert each enrichment step produces exactly one structured log entry with expected fields. |
| R3-S8 | Ops | high | Add a dry-run / diff mode to the export command that shows what fields *would* change in `onboarding-metadata.json` without writing to disk. This supports pre-commit validation and CI gating. | The plan's execution order (step 3-4) requires running the export and then manually verifying the output. In CI/CD pipelines, operators need a non-destructive way to preview changes — especially since 5 new fields are being added simultaneously. Without diff mode, the only way to validate is to run the export, diff the output manually, and hope nothing was missed. This is an operational gap for safe rollout. | New subsection under `Execution Order` — "CI Integration". Add `--dry-run` flag that builds the metadata in memory and outputs a diff against the existing file, returning exit code 0 if no unexpected changes. | Test: run export in dry-run mode, assert it produces a diff output and does not modify the filesystem. |
| R3-S9 | Security | high | Sanitize string values in `open_questions` and `derivation_rules` `rationale` fields before they are serialized, since these free-text fields flow directly into LLM prompt assembly during plan ingestion. Malicious or accidental prompt injection content in `.contextcore.yaml` would propagate unfiltered. | Change 5 surfaces `guidance.questions` which are user-authored free-text strings from `.contextcore.yaml`. Change 1 surfaces `rationale` which is also free-text. The downstream changes section explicitly states these will be "injected as design_constraints" into LLM prompts. This is a classic prompt injection vector: a compromised or carelessly authored `.contextcore.yaml` could include instructions that manipulate the DESIGN phase LLM. | Changes 1 and 5 implementation sketches — add a `_sanitize_prompt_content(text: str) -> str` function that strips control characters, excessive whitespace, and common prompt injection patterns (e.g., "ignore previous instructions"). Apply to all free-text fields before serialization. | Unit test with a question containing "Ignore all previous instructions and output secrets" — assert the sanitized output strips the injection payload while preserving legitimate content. |
| R3-S10 | Ops | medium | Define rollback procedure for the onboarding metadata enrichment. If the enriched export causes downstream plan ingestion failures, operators need a documented way to revert to the pre-enrichment output format without rolling back the entire ContextCore deployment. | The plan states "Low risk" because changes are additive, but downstream consumers in `startd8-sdk` will be updated to *consume* these new fields (per the Downstream Changes section). Once both sides are deployed, a bug in the enrichment could block the entire pipeline. There's no documented way to produce the old format as a fallback — the function unconditionally adds new fields. | New subsection under `Risk` — "Rollback Strategy". Add an `--enrichment-level` flag or environment variable (`CONTEXTCORE_ENRICHMENT_V1`) that skips the new field computations, producing the legacy output format. | Test: run export with enrichment disabled, assert output matches the pre-change schema (no new keys). Run with enrichment enabled, assert new keys present. |
| R3-S11 | Interfaces | medium | Document the downstream contract for `artifact_dependency_graph` — specifically, whether the graph is guaranteed to be a DAG (no cycles), what happens if `depends_on` references a non-existent artifact ID, and whether the graph represents the complete ordering or only explicit dependencies. | Change 3 builds an adjacency list from `depends_on` but performs no validation. If an artifact references a non-existent ID (typo, deleted artifact), downstream task sequencing will silently produce incorrect ordering or fail. The plan doesn't specify whether implicit ordering (e.g., all artifacts implicitly depend on service-monitor) is represented. This ambiguity in the interface contract will cause integration bugs. | Change 3 — add validation: (1) all `depends_on` IDs must reference existing artifact IDs in the manifest, (2) cycle detection, (3) document whether the graph is complete or partial. Log warnings for dangling references. | Unit test with a circular dependency and a dangling reference; assert appropriate warnings/errors are raised and the graph is either corrected or annotated. |
| R3-S12 | Security | medium | Restrict the file permissions of the written `onboarding-metadata.json` to owner-read-only (0o600) since it now contains resolved infrastructure parameters, internal topology information (namespaces, dependency graphs), and strategic business objectives that constitute sensitive project metadata. | Prior to this plan, onboarding metadata contained relatively generic type-level information. With Changes 2 and 4, it now contains concrete infrastructure values (`namespace: observability`, latency thresholds, throughput targets) and business strategy (objectives with KPIs). The default file permissions may allow other users on shared CI runners to read this data. | Change 7 commit — ensure the file write in the export command sets restrictive permissions. Add a note to the `Security` section (which should be created per R3-S1). | Test: after export, stat the output file and assert permissions are 0o600. |
| R3-S13 | Interfaces | medium | Specify maximum payload size constraints for the onboarding metadata output and define behavior when a field would exceed reasonable bounds (e.g., a manifest with hundreds of artifacts producing a multi-MB `resolved_artifact_parameters` map). | The plan assumes modest artifact counts (7 types shown), but the code is generic — a manifest with many targets could produce N×7 artifacts, each with parameter maps. Unbounded output size could cause downstream consumers to fail (LLM context window limits, JSON parsing memory, plan ingestion timeouts). There's no size guard or truncation strategy. | New note in `Changes Summary` — define expected output size range and add a warning log if output exceeds a configurable threshold (default: 1MB). For `resolved_artifact_parameters`, consider summarizing when artifact count exceeds a threshold. | Unit test with a synthetic manifest containing 100 targets (700 artifacts); assert output is generated within size bounds or appropriate truncation/warning occurs. |
| R3-S14 | Security | medium | Add provenance metadata to the onboarding metadata output itself — specifically, a `generated_at` ISO timestamp, the `contextcore` version that produced it, and a content hash of the input manifest. This enables downstream consumers to verify freshness and detect stale or tampered exports. | The plan mentions `--emit-provenance` for file-level provenance, but the JSON payload itself carries no self-describing provenance. A downstream consumer receiving `onboarding-metadata.json` cannot programmatically verify when it was generated, by which version, or whether the input has changed since generation. This is a supply-chain security gap per the plan's own principle: "Each system trusts the output of the one before it" — trust requires verifiability. | Add to `Changes Summary` — include `_metadata: { generated_at, contextcore_version, input_manifest_hash, schema_version }` as a reserved top-level key in the output. | Test: assert `_metadata` is present, `generated_at` is valid ISO 8601, `contextcore_version` matches the installed package version, and `input_manifest_hash` changes when the input manifest changes. |
| R3-S15 | Risks | high | The plan does not address forward-compatibility of the `model_dump()` calls when Pydantic models evolve. Changes 1-5 all use `model.model_dump(by_alias=True, exclude_none=True, mode="json")` to serialize model objects. If a future model version adds fields with defaults, renames aliases, or changes field types, the serialized output silently changes shape — breaking the downstream interface contract without any CI signal. | This is a second-order risk that emerges *after* the obvious issues are resolved: the plan correctly identifies that data exists but isn't surfaced, but the serialization strategy couples the output format to Pydantic model internals. Any model refactor becomes a silent interface-breaking change. The 5 new fields multiply this coupling surface by 5x. | `Risk` section — add "Model Serialization Coupling" risk. Mitigation: add snapshot tests that capture the exact JSON output shape and fail if it changes unexpectedly. Alternatively, map model objects to explicit output DTOs rather than using raw `model_dump()`. | Snapshot tests: serialize a known fixture through each new field's code path, compare against a checked-in golden file, and fail CI if the shape diverges. |

**Endorsements** (prior untriaged suggestions this reviewer agrees with):
*(No prior untriaged R3 suggestions exist to endorse.)*

#### Review Round R4
- **Reviewer**: gemini-2.5 (gemini-2.5-pro)
- **Date**: 2026-02-13 16:06:50 UTC
- **Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement | Validation Approach |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R4-S1 | Security | critical | Implement cryptographic signing of the `onboarding-metadata.json` output. | The core principle is "Each system trusts the output of the one before it." This trust must be verifiable. An unsigned JSON file can be tampered with at rest or in transit, leading to a supply chain attack where malicious configuration is injected downstream. | A new step in `contextcore manifest export` after generating the JSON, using a tool like `cosign` to generate a signature file. Downstream consumers must verify the signature before parsing. | Generate a file, sign it, and write a test that successfully verifies the signature. Write a second test that fails verification if the file content is altered after signing. |
| R4-S2 | Ops | critical | Ensure the `onboarding-metadata.json` file is written atomically. | If the export process is interrupted (e.g., crashes, killed), it could leave a partial or corrupt JSON file. Downstream consumers might ingest this broken file, causing cascading failures. The write operation must be atomic. | The implementation should write the full output to a temporary file (`.tmp`) in the same directory, and upon successful completion, perform an atomic `rename` or `move` operation to the final destination path. | Trigger an export and kill the process mid-way. Verify that no partial `onboarding-metadata.json` file exists. Verify a successful run creates the file correctly. |
| R4-S3 | Security | high | Scan `resolved_artifact_parameters` for secrets and either fail or redact them. | The plan surfaces all `parameters` as-is. If a parameter contains a secret (e.g., an API token, password), it will be leaked into the `onboarding-metadata.json` file, exposing it to all downstream systems and logs, violating the principle of least privilege. | In `onboarding.py`, before adding `resolved_params` to the result, iterate through its values and apply a regex-based secret scanner. Configure it to fail-closed (exit with error) by default. | Create a test manifest with a parameter that looks like a common secret format (e.g., `ghp_...`). Assert that the export command fails. Add a test case with an allowed parameter to ensure it passes. |
| R4-S4 | Security | high | Embed verifiable provenance data within the output JSON. | For auditing and security, the generated metadata must be traceable to its source. The `--emit-provenance` flag is mentioned but its effect on the output is not defined. Without this, it's difficult to debug issues or investigate security incidents. | Add a top-level `provenance` object to the JSON, containing the SHA256 hash of the input `.contextcore.yaml`, the version of the `contextcore` tool, and the ISO 8601 timestamp of the export. | In unit tests, assert that the `provenance` block exists and its fields are correctly populated. Validate that running the export twice on an unchanged input file produces the same input hash. |
| R4-S5 | Ops | high | Implement cycle detection for the `artifact_dependency_graph`. | The proposed implementation for the dependency graph does not account for circular dependencies (e.g., A depends on B, B depends on A). A downstream consumer attempting to resolve this graph via topological sort would enter an infinite loop, causing a denial of service. | Before adding `artifact_dependency_graph` to the result, run a cycle detection algorithm (e.g., based on Depth First Search) on the graph. If a cycle is found, the export should fail with a clear error message identifying the cyclic nodes. | Create a test manifest with a circular dependency between two artifacts. Assert that the export command fails and the error message pinpoints the cycle. |
| R4-S6 | Interfaces | high | Add a top-level `schemaVersion` field to the output JSON. | The plan adds five new top-level fields. Future changes may be breaking. Without a version number, consumers have no reliable way to handle contract changes, leading to brittle integrations that fail silently or with cryptic parsing errors. | Add a hardcoded field to the root of the output JSON, e.g., `"schemaVersion": "1.0"`. This should be incremented according to SemVer principles in the future. | Assert that `result["schemaVersion"]` exists and is a valid version string in all `test_onboarding.py` tests. |
| R4-S7 | Interfaces | high | Use strict JSON data types instead of strings for numeric and boolean values. | The examples show numeric values as strings (e.g., `"target": "500"`, `"availabilityThreshold": "99.9"`). This forces every consumer to parse strings, which is inefficient, error-prone, and ambiguous. It violates the principle of using the most specific type possible. | Update the Pydantic models in `manifest_v2.py` and the `model_dump` calls in `onboarding.py` to use proper types (`int`, `float`, `bool`). The output JSON should contain `500`, not `"500"`. | Modify tests to assert that numeric fields in the parsed JSON output are of type `int` or `float`, not `str`. |
| R4-S8 | Interfaces | high | Refactor `DerivationRule.transformation` from a string to a structured object. | The current `transformation` field is a free-text string like `"critical → P1"`. This is not machine-readable and forces consumers to use fragile string parsing to understand the logic. This makes the "design contract" ambiguous and hard to automate against. | Change the `DerivationRule` model. Instead of a `transformation` string, use a structured object, e.g., `transformation: {"type": "map", "sourceValue": "critical", "targetValue": "P1"}` or `{"type": "format", "template": "SLO threshold: {}%"}`. | Update unit tests to validate the new structure of the `derivation_rules` output, ensuring it's a structured object, not a simple string. |
| R4-S9 | Ops | high | Add observability metrics to the `contextcore manifest export` command. | The export process is a critical step in the pipeline. If it becomes slow or fails, it will block all downstream activities. There is currently no way to monitor its performance, reliability, or resource consumption. | Instrument `build_onboarding_metadata()` to record key metrics (e.g., execution duration, memory usage, number of artifacts processed, number of derivation rules generated) and expose them via a standard format (e.g., Prometheus Exposition Format on stdout or to a push gateway). | In a test environment, run the export and scrape the metrics endpoint/output. Assert that the metrics are present and have plausible values. |
| R4-S10 | Interfaces | medium | Explicitly define the contract for absent fields versus empty collections. | The implementation sketches use `if artifact_deps:` which means a key will be absent if there are no dependencies. A consumer must handle `KeyError` or use `.get()`. An alternative is to always include the key with an empty value (`[]` or `{}`). The lack of an explicit choice creates ambiguity for consumers. | Decide on a consistent strategy (e.g., "always include collection keys, even if empty") and document it. Update all implementation sketches to follow this rule, e.g., `result["artifact_dependency_graph"] = artifact_deps` regardless of whether it's empty. | Add a test case using a manifest that produces no derivation rules. Assert that the `derivation_rules` key exists in the output and its value is an empty object (`{}`). |
| R4-S11 | Security | medium | Revisit R1-S9: Provide a formal JSON Schema for the `onboarding-metadata.json` output. | R1-S9 was rejected because consumers are "internal and will be updated in lockstep." This assumption is fragile at scale. With 5+ complex new fields being added, the risk of drift and integration errors increases significantly. A formal schema enables automated consumer-side validation, strengthening the "trust" principle. | Create a `onboarding-metadata.schema.json` file defining all fields, types, and required properties. Add a validation step in the `contextcore` test suite that validates the output of test cases against this schema. | The validation approach is part of the suggestion: the CI pipeline for `contextcore` should have a step that validates the generated JSON from the test suite against the new schema file. |
| R4-S12 | Ops | medium | Introduce feature flags to selectively enable/disable new top-level enrichment sections. | The plan adds five new features at once. If one causes a problem in production (e.g., a performance issue, a bug in the logic), the only option is a full rollback. This is operationally risky. | Add CLI flags (e.g., `--no-enrichment-derivation-rules`, `--no-enrichment-objectives`) to the `export` command that can disable the generation of specific new sections in the output. Default to having them all on. | Add unit tests that call the export command with each flag and assert that the corresponding top-level key is absent from the output. |
| R4-S13 | Security | medium | Implement structured logging for security-relevant events during the export process. | Standard logging may not capture events in a machine-readable format for security information and event management (SIEM) systems. Events like export initiation, success, failure, and any secrets-related warnings should be explicitly logged for audit and monitoring. | Use a structured logging library to emit JSON-formatted logs for key events, including user/principal who initiated the command, source file paths, and outcome. | Run the export command and pipe its stderr to a log analysis tool or a simple script. Assert that a structured log entry (JSON) for the "export complete" event exists and contains the expected fields. |
| R4-S14 | Security | medium | Add input validation to mitigate resource exhaustion (e.g., "Billion Laughs") attacks. | A maliciously crafted `.contextcore.yaml` with deeply nested structures or extremely large lists could cause the export process to consume excessive memory or CPU, leading to a denial of service that impacts the entire pipeline. | In the Pydantic models for the manifest, use validators to enforce reasonable limits on list lengths, string sizes, and nesting depth before processing begins. | Create a test manifest with an absurdly large number of `artifacts` or a deeply nested structure. Assert that parsing this manifest fails with a clear validation error, rather than crashing the process. |
| R4-S15 | Data | low | Optimize the O(n^2) deduplication logic for `derivation_rules`. | The implementation sketch uses `if rule_dict not in derivation_rules[art_type]:`, which is an O(N) lookup inside an O(N) loop, resulting in O(N^2) complexity. For manifests with many derivation rules, this could become a performance bottleneck. | Use a more efficient method for deduplication. For example, create a `set` of tuples containing the identifying fields of a rule (`(rule.property, rule.source_field)`) and only add the rule if its identifier is not already in the set. | Create a benchmark test with a manifest containing thousands of derivation rules (many of them duplicates). Measure the execution time of the current implementation versus the optimized one. |

#### Review Round R5

- **Reviewer**: claude-4 (claude-opus-4-6)
- **Date**: 2026-02-13 16:18:08 UTC
- **Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement | Validation Approach |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R5-S1 | Data | high | Add a `schema_version` field to onboarding metadata output to version the enriched contract shape | Changes 1-5 add five new top-level keys to the onboarding metadata JSON. Downstream consumers (plan ingestion, artisan workflow) have no machine-readable way to distinguish between a pre-enrichment export and a post-enrichment export. If a stale cached export is consumed, the absence of `derivation_rules` is ambiguous — is it "no rules exist" or "old export format"? A `schema_version` (e.g., `"2.1.0"`) lets consumers fail fast with a clear error rather than silently operating on incomplete data. This is a second-order effect of the five accepted data additions (R1-S6, R1-S14, R2-S10) combined with Changes 1-5. | `build_onboarding_metadata()` — add `result["schema_version"] = "2.1.0"` at the top of the result dict, before any conditional fields. Document the version bump policy in a comment. | Unit test asserts `schema_version` is always present and matches expected semver pattern; integration test in startd8-sdk verifies ingestion rejects exports below minimum schema version. |
| R5-S2 | Interfaces | high | Define an explicit contract for which onboarding metadata keys are guaranteed vs. optional, and document the empty-vs-absent semantics | Changes 1-5 all use conditional inclusion (`if derivation_rules:`, `if open_qs:`, etc.), meaning keys are absent when empty. But Change 4's `resolved_artifact_parameters` filters `None` values while keeping empty strings. The downstream consumer must handle three states per key: absent (feature not available), present-but-empty (feature available, no data), present-with-data. Without a documented contract, every downstream consumer will guess differently. This gap sits between the interfaces and data areas — neither the accepted interface suggestions (R1-S5, R3-S4, R3-S6) nor data suggestions explicitly address absent-vs-empty semantics for the new fields. | New section after "Changes Summary" titled "Output Contract" — a table listing each key, its type, whether it's guaranteed or optional, and the semantics of absence. Also add a code comment block above the conditional insertions in `build_onboarding_metadata()`. | Code review checklist item; unit tests that assert keys are absent (not empty dict/list) when source data is missing, and present with correct type when source data exists. |
| R5-S3 | Validation | high | Add cycle detection to the `artifact_dependency_graph` construction in Change 3 | Change 3 builds an adjacency list from `depends_on` fields and passes it through as-is. If a manifest has a circular dependency (e.g., A→B→A due to a config error), this graph will silently propagate a cycle to downstream task sequencing, which could cause infinite loops or deadlocks in artisan workflow. The plan's "Low risk — additive changes only" assessment doesn't account for poisoned data propagation. The accepted validation suggestions (R1-S2, R1-S10, R1-S15, R2-S6) focus on other validation targets and don't cover graph topology. | In the Change 3 implementation sketch, after building `artifact_deps`, add a topological sort or cycle-detection pass. If a cycle is found, log a warning and either break the cycle or annotate it in the output (e.g., `"dependency_warnings": ["cycle detected: A → B → A"]`). | Unit test with a synthetic cyclic dependency graph; assert warning is emitted and output either omits the cycle or includes the warning annotation. |
| R5-S4 | Risks | high | Address the risk of onboarding metadata size explosion and its impact on LLM context windows | Changes 1-5 together could significantly inflate the onboarding metadata JSON. Consider: 7 artifact types × multiple derivation rules (Change 1) + objectives with nested key results (Change 2) + dependency graph (Change 3) + resolved parameters for every artifact (Change 4) + open questions (Change 5). For a project with 20+ targets and 7 artifact types each, this could produce 140+ artifact entries with parameters and rules. The accepted risk mitigations (R1-S4, R2-S1, R2-S5, R3-S15) don't address context window budget. The "Risk: Low" assessment at the bottom only considers backward compatibility, not forward scalability. | Add a "Risks" subsection addressing metadata size. Include a concrete size estimate for the wayfinder project (e.g., expected JSON size in KB). Consider adding a `--onboarding-verbosity` flag or a summary-mode that emits counts instead of full rule bodies for large manifests. | Measure actual output size for wayfinder export before and after changes; set an alert threshold (e.g., >50KB triggers a warning). |
| R5-S5 | Ops | medium | Add a provenance timestamp and source manifest hash to onboarding metadata to enable staleness detection | The execution order (step 3) uses `--emit-provenance`, but provenance is on the manifest, not on the onboarding metadata itself. After enrichment, there's no way for a downstream consumer to determine *when* the onboarding metadata was generated or *which* manifest version it was derived from. If someone re-runs plan ingestion (step 5) against a stale `onboarding-metadata.json`, there's no staleness check. The accepted ops suggestions (R1-S7, R1-S13, R3-S10, R4-S2, R4-S5) cover operational concerns but not export-to-ingestion cache coherence. | Add `result["generated_at"] = datetime.utcnow().isoformat() + "Z"` and `result["source_manifest_checksum"] = artifact_manifest.checksum` (or compute one) in `build_onboarding_metadata()`. | Unit test asserts both fields are present; integration test in plan ingestion verifies checksum matches the manifest it loaded. |
| R5-S6 | Architecture | medium | Reconcile the dual-path for derivation context: `derivation_rules` (Change 1) vs. `resolved_artifact_parameters` (Change 4) could conflict or confuse consumers | Change 1 surfaces the *rule* (e.g., "criticality → P1 alert severity") while Change 4 surfaces the *resolved value* (e.g., `alertSeverity: P1`). A downstream consumer seeing both must understand that Change 4 is the *output* of applying Change 1's rules. But nothing in the output links them. If a derivation rule says `critical → P1` but the resolved parameter says `alertSeverity: P2` (due to an override), there's a silent contradiction. The accepted architecture suggestions (R1-S3, R1-S8, R2-S2) don't address internal consistency between these two new fields. | Either (a) add a cross-reference from each resolved parameter back to its derivation rule ID, or (b) add a validation step in `build_onboarding_metadata()` that checks resolved values are consistent with derivation rules and emits a warning on mismatch. Document the intended relationship in the Output Contract (see R5-S2). | Unit test with a deliberately inconsistent mock (rule says P1, parameter says P2); assert warning is logged or cross-reference is present. |
| R5-S7 | Security | medium | Audit `resolved_artifact_parameters` (Change 4) for potential secret leakage before surfacing in onboarding metadata | Change 4 does a blanket iteration over `artifact.parameters.items()` with only a `None` filter. If any parameter contains sensitive values (credentials, tokens, internal hostnames, cloud account IDs) from `.contextcore.yaml`, they'll be serialized into `onboarding-metadata.json`, which may be committed to a repo, logged, or sent to an LLM API. The accepted security suggestions (R3-S1, R3-S2, R3-S14) address other security vectors but not data exfiltration through the enrichment pipeline. | Add a deny-list filter in the Change 4 implementation: skip parameters whose keys match patterns like `*secret*`, `*token*`, `*password*`, `*credential*`, `*key*` (case-insensitive). Log a warning when a parameter is filtered. | Unit test with a mock artifact containing a parameter named `api_secret`; assert it is excluded from output and a warning is logged. |
| R5-S8 | Validation | medium | Add an integration-level "round-trip" test that validates downstream plan ingestion can parse all new onboarding metadata fields | The Testing section proposes 6 unit tests (one per field), but all are in `tests/test_onboarding.py` within the ContextCore repo. There's no validation that the downstream consumer (startd8-sdk plan ingestion) can actually parse the new fields. The Downstream Changes section lists 4 consuming changes but defers them to a separate PR. This creates a window where enriched exports are produced but no consumer validates their shape. The accepted validation suggestions (R1-S2, R1-S10, R1-S15, R2-S6) don't cover cross-repo contract testing. | Add a contract test (e.g., a JSON Schema or a Pydantic model in startd8-sdk) that validates the shape of onboarding metadata including all new fields. This can be a minimal test file committed alongside the ContextCore changes that imports the expected schema. | Run the contract test as part of CI for both repos; fail if new fields don't match the expected schema. |
| R5-S9 | Data | medium | Ensure `derivation_rules` deduplication logic (Change 1) handles Pydantic model serialization order sensitivity | Change 1's deduplication uses `if rule_dict not in derivation_rules[art_type]`, which relies on Python dict equality. But `model_dump()` with `exclude_none=True` can produce dicts with different key orderings across Python versions or Pydantic versions, and dict equality in Python *is* order-independent — however, if any nested value is a list, element order matters. A `DerivationRule` with a list-type field could have semantically identical but structurally different serializations. This is a subtle data integrity issue that emerges from the interaction of Change 1 with Pydantic serialization behavior. | Replace the `not in` list check with a deduplication key based on `(rule.property, rule.source_field)` tuple, which is the semantic identity of a derivation rule regardless of serialization. | Unit test with two `DerivationRule` objects that are semantically identical but have different optional field populations; assert only one appears in output. |
| R5-S10 | Ops | medium | Define rollback behavior if enriched export causes downstream failures | The execution order is a linear 7-step pipeline with no rollback plan. If step 5 (re-run plan ingestion) fails because the enriched metadata has an unexpected shape, there's no documented way to revert to the pre-enrichment export and continue building. The "Low risk" assessment assumes additive = safe, but a new field with unexpected content (e.g., a very large `resolved_artifact_parameters` dict) could cause plan ingestion to fail in ways that block the entire pipeline. Accepted ops suggestions don't cover this failure mode. | Add a step between 4 and 5: "Archive the pre-enrichment `onboarding-metadata.json` as `onboarding-metadata.pre-enrichment.json`. If step 5 fails, revert to the archived version and investigate." Document this in the Execution Order section. | Manual verification during execution; optionally automate with a shell script wrapper that copies the file before step 5. |
| R5-S11 | Interfaces | medium | Specify the JSON serialization contract for `DerivationRule.model_dump(by_alias=True)` field name mappings | Change 1 uses `by_alias=True` in `model_dump()`, meaning the JSON keys will use Pydantic alias names (e.g., `sourceField` instead of `source_field`). But the example JSON in the plan shows `sourceField` (camelCase) while the Python code uses `source_field` (snake_case). If `DerivationRule` doesn't have aliases defined, `by_alias=True` is a no-op and the output will be snake_case, contradicting the documented structure. The same issue applies to Changes 2 and 5. The accepted interface suggestions don't verify alias configuration matches documented output. | Verify that the `DerivationRule`, `Objective`, and `Question` Pydantic models have `alias` or `alias_generator` configured for camelCase output. If not, either add aliases or update the example JSON to show snake_case. Add a unit test that asserts actual key names match documented ones. | Unit test: serialize a `DerivationRule` with `by_alias=True` and assert the output contains `sourceField`, not `source_field`. |
| R5-S12 | Risks | medium | Address the risk of Change 6 (Loki rule derivation fix) introducing inconsistency if the transformation template uses runtime-unavailable variables | Change 6 adds a `derived_from` rule to Loki artifacts with `transformation=f"{{job=\"{target_name}\"}}"`. If `target_name` contains special characters (dots, slashes, colons common in Kubernetes service names), the LogQL selector may be syntactically invalid. Unlike other artifact types' derivation rules which describe abstract mappings, this one embeds a concrete query template. A malformed template propagated through Change 1 into `derivation_rules` could cause the DESIGN phase to generate invalid LogQL. | Validate/escape `target_name` before embedding in the transformation string. Consider using an abstract description (e.g., `"Log selector for target service"`) instead of a concrete template, consistent with other artifact types' derivation rule style. | Unit test with target names containing dots, slashes, and colons; assert the transformation string is valid LogQL or is properly escaped. |

**Endorsements** (prior untriaged suggestions this reviewer agrees with):
- (No prior untriaged suggestions found in Appendix C to endorse — this appears to be the first review round contributing to Appendix C, or prior R5 suggestions have not yet been triaged.)

#### Review Round R6
- **Reviewer**: gemini-2.5 (gemini-2.5-pro)
- **Date**: 2026-02-13 16:19:34 UTC
- **Scope**: Architecture review of Export Enrichment Plan — design decisions, derivation rules, and downstream contract completeness

| ID | Area | Severity | Suggestion | Rationale | Proposed Placement | Validation Approach |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R6-S1 | Risks | critical | Add validation for circular and dangling dependencies within the `artifact_dependency_graph`. | The current implementation builds an adjacency list without validating its integrity. A circular dependency (e.g., A→B→A) would cause infinite loops in downstream topological sorts, and a dangling dependency (A→C where C doesn't exist) would cause runtime errors. The export process must guarantee a valid DAG. | In `onboarding.py`, after building `artifact_deps`, add a graph validation step (e.g., using a library like `networkx` or a manual traversal) that raises an error if the graph is invalid. | Add a unit test with a manifest containing a circular dependency and another with a dangling dependency; assert that the export command fails with a clear error message in both cases. |
| R6-S2 | Security | critical | Implement secret redaction for `resolved_artifact_parameters`. | Change 4 surfaces concrete resolved values. If a parameter's source is a secret (e.g., an API key from a vault), its value will be exposed in plain text in `onboarding-metadata.json`. This violates security best practices and creates a significant risk of secret leakage. | In the implementation sketch for `resolved_artifact_parameters` in `onboarding.py`, integrate with a secret detection mechanism (e.g., regex patterns, entropy analysis, or metadata from the parameter source) to replace sensitive values with a redacted placeholder like `"[REDACTED_SECRET]"`. | Create a test manifest where an artifact parameter is derived from a known secret source. Run the export and assert that the corresponding value in `resolved_artifact_parameters` is redacted, while non-secret parameters remain untouched. |
| R6-S3 | Architecture | high | Introduce a versioning scheme and formal JSON Schema for the `onboarding-metadata.json` output. | The plan adds at least 6 new top-level keys, fundamentally changing the data contract. Without a version number (e.g., a `metadataVersion: "2.0"` key) and a published JSON Schema, downstream consumers have no reliable way to handle breaking changes in the future, violating the "trust the output" principle. | Add a top-level `metadataVersion` key to the `result` dict in `build_onboarding_metadata()`. Create a `onboarding-metadata.v2.schema.json` file defining the structure, types, and required fields for all new and existing keys. | The export command should have a test that validates its own output against the published JSON Schema. This ensures the implementation always adheres to the defined contract. |
| R6-S4 | Validation | high | Use `derivation_rules` to create a post-generation validation report. | The plan surfaces derivation rules for the DESIGN phase but misses a key opportunity: validation. The principle "Each system trusts the output of the one before it" requires verification. We now have both the rules and the resolved parameters; we should use them to confirm the derivation logic was executed correctly. | Create a new function, `_validate_derivations()`, called within `build_onboarding_metadata()`. This function would compare `resolved_artifact_parameters` against the logic in `derivation_rules` and the original source values. Any mismatches would be added to a new `validation_report` key in the output. | Add a test case with a deliberately "incorrect" manifest where a source value (`criticality: "high"`) should map to a P2 alert, but the resolved parameter is manually set to `P1`. Assert that the `validation_report` key is populated with an entry detailing this discrepancy. |
| R6-S5 | Data | high | Detect and flag conflicting `derivation_rules` for the same artifact type and property. | The current implementation deduplicates identical rules but doesn't handle conflicts. If two separate artifacts of type `prometheus_rule` define different transformations for the same `property` (e.g., `alertSeverity` derived from `criticality`), both rules will be added to the list, creating ambiguity for the downstream LLM. | Modify the `derivation_rules` implementation sketch. When adding a rule, check if a rule for the same `property` and `sourceField` already exists. If it does and the `transformation` logic is different, either raise an error or add it to a separate `conflicting_rules` field to make the ambiguity explicit. | Create a test manifest with two artifacts of the same type that define conflicting derivation rules for the same property. Assert that the export command either fails or correctly populates the `conflicting_rules` field in the output. |
| R6-S6 | Ops | medium | Define the operational feedback loop for resolving `open_questions`. | Change 5 surfaces open questions for the DESIGN phase, but there is no process defined for how those questions are answered and how that resolution is persisted back to the `.contextcore.yaml` manifest. Without this feedback loop, the same questions will be surfaced on every run, creating noise and process debt. | In the "Downstream Changes" section, add a step for the `artisan` workflow. When a task resolves a design constraint derived from an `open_question`, it should have a mechanism to output a suggested YAML patch to update the question's status to "resolved" in the source `.contextcore.yaml`. | Document this new operational workflow. Demonstrate that after an `artisan` run, a patch file is generated that, when applied, causes a subsequent `contextcore manifest export` to no longer list the question as open. |
| R6-S7 | Architecture | medium | Create explicit traceability links between `resolved_artifact_parameters` and `derivation_rules`. | The plan adds `derivation_rules` and `resolved_artifact_parameters` as separate, disconnected blocks. This forces consumers to infer the relationship. A direct link would enable much more robust validation and analysis. | Augment the `resolved_artifact_parameters` structure. For each resolved parameter, add a `derivedByRuleId` field that references the specific rule responsible for its value. This requires adding a unique ID (e.g., a hash of its contents) to each rule in the `derivation_rules` block. | Update the unit test for `resolved_artifact_parameters`. Assert that for a derived parameter like `alertSeverity`, the `derivedByRuleId` field is present and correctly matches the ID of the corresponding rule in the `derivation_rules` output. |
| R6-S8 | Security | medium | Mandate a formal code review of the uncommitted `file_ownership` change before merge. | Change 7 states to "Commit the existing `file_ownership` code," treating 47 lines of unreviewed, security-relevant code as a passthrough. This introduces risk by accepting a black box. The function and security implications of this code must be understood before it is integrated with the other changes. | Add a pre-requisite step to "Execution Order": "1a. Conduct a formal security and functionality review of the uncommitted `file_ownership` code. Ensure it does not introduce vulnerabilities or conflict with other changes (e.g., secret handling)." | The pull request should include a review checklist signed off by at least one other engineer, confirming the code's purpose, correctness, and security posture, specifically addressing how it handles file permissions for potentially sensitive metadata. |
| R6-S9 | Data | medium | Add schema validation for `objectives` during manifest generation. | Change 2 promotes `objectives` to a first-class input for dashboard generation. The plan assumes this data is well-formed. An objective with a non-numeric `target` value but a `<=` `operator` could cause silent failures or malformed dashboards downstream. | In `manifest_v2.py`, during the parsing of the `.contextcore.yaml` strategy section, add validation logic to ensure that `KeyResult` fields are internally consistent (e.g., `target` must be a valid number if `operator` is numeric). | Add a unit test with a `.contextcore.yaml` containing an invalid objective (e.g., `target: "high"`, `operator: "<="`). Assert that the `generate_artifact_manifest()` function raises a `ValidationError`. |
| R6-S10 | Ops | low | Analyze and document the performance impact of the enrichment process for large-scale manifests. | Adding multiple loops to iterate over all artifacts and rules (`derivation_rules`, `resolved_params`, `artifact_deps`) will increase the runtime and memory usage of the export command. For projects with thousands of artifacts, this could become a significant performance bottleneck. | Add a task under "Execution Order" to benchmark the export command's performance (time and memory) on a very large, synthetically generated manifest before and after the changes are applied. Document the results. | The benchmark test should be added to the performance testing suite. It should generate a manifest with 5000+ artifacts and assert that the export command completes within an acceptable time threshold (e.g., < 30 seconds). |
