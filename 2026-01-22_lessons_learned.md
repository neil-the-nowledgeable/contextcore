# Lessons Learned - ContextCore Development

This document captures lessons learned during ContextCore development, organized by date for easy reference and appending.

---

## 2026-01-22: TUI Implementation & startd8 SDK Integration

### Code Extraction from AI-Generated Content

**Problem:** When extracting code from markdown code blocks generated by AI workflows, file markers can appear inside the code blocks rather than outside them.

**Solution:** 
- Use regex patterns that look for file markers (`# === FILE: path ===`) inside code blocks
- Handle both Python (`#`) and CSS (`/* */`) style markers
- Strip markers from extracted code before saving
- Provide fallback extraction for code blocks without markers

**Example Pattern:**
```python
# Pattern: ```python\n# === FILE: path ===\n...code...```
code_block_pattern = r'```(python|css|tcss)\n(.*?)```'
file_marker_pattern = r'(?:#|/\*)\s*===\s*FILE:\s*([^\s=]+(?:\s+[^\s=]+)*)\s*===\s*(?:\*/)?'
```

**Key Takeaway:** Always validate and handle multiple formats when parsing AI-generated content. Don't assume a single consistent format.

---

### Defensive Programming for External SDK Integration

**Problem:** When integrating with external SDKs (like startd8), workflow results may have `None` values for attributes, causing `AttributeError` when accessing nested properties.

**Solution:** Implement defensive checks at every level:
1. Check if result object exists
2. Check if attributes exist before accessing
3. Use `.get()` with defaults for dictionaries
4. Validate types before operations
5. Wrap risky operations in try/except

**Pattern:**
```python
# Bad: Direct access (can fail)
cost = result.metrics.total_cost

# Good: Defensive access
if hasattr(result, 'metrics') and result.metrics:
    try:
        cost = result.metrics.total_cost if hasattr(result.metrics, 'total_cost') else 0
    except (AttributeError, TypeError):
        cost = 0
else:
    cost = 0
```

**Key Takeaway:** When integrating external SDKs, assume nothing about return value structure. Always validate before access.

---

### Error Handling Strategy for Batch Processing

**Problem:** When processing multiple features in a batch, one failure should not stop the entire process.

**Solution:**
- Wrap each feature processing in try/except
- Collect failed results with error messages
- Continue processing remaining features
- Provide summary statistics at the end

**Pattern:**
```python
results = []
for feature in features:
    try:
        result = process_feature(feature)
        results.append(result)
    except Exception as e:
        # Log error but continue
        results.append({
            "success": False,
            "error": str(e),
            # ... other fields with defaults
        })
        continue

# Summary
print(f"Successful: {sum(1 for r in results if r['success'])}")
print(f"Failed: {sum(1 for r in results if not r['success'])}")
```

**Key Takeaway:** Design batch operations to be resilient. One failure shouldn't prevent learning from other successes.

---

### File I/O Error Handling

**Problem:** File operations can fail for many reasons (permissions, disk space, path issues), and failures should be handled gracefully.

**Solution:**
- Wrap each file operation in try/except
- Provide specific error messages
- Continue processing other files even if one fails
- Use safe defaults for missing data

**Pattern:**
```python
try:
    with open(file_path, "w") as f:
        f.write(content)
    print(f"Saved: {file_path}")
except Exception as e:
    print(f"Error saving {file_path}: {e}")
    # Continue with next file
```

**Key Takeaway:** File I/O is inherently risky. Always handle failures gracefully and provide actionable error messages.

---

### Type Validation Before String Operations

**Problem:** Assuming variables are strings can cause `TypeError` when they're `None` or other types.

**Solution:** Validate types before string operations:
```python
# Bad: Assumes string
text = result["final_implementation"]
files = extract_code_blocks(text)

# Good: Validates type
final_implementation = result.get("final_implementation", "")
if final_implementation and isinstance(final_implementation, str):
    files = extract_code_blocks(final_implementation)
```

**Key Takeaway:** Always validate types before operations, especially when data comes from external sources or APIs.

---

### Documentation for Error Scenarios

**Problem:** When fixing errors, it's easy to forget what was fixed and why, making future debugging harder.

**Solution:** Create a `*_FIXES.md` document that:
- Lists all error scenarios handled
- Explains the fix for each
- Provides testing recommendations
- Documents expected behavior

**Key Takeaway:** Document fixes immediately while context is fresh. Future you (and others) will thank you.

---

### startd8 SDK Dependency Management

**Problem:** The startd8 SDK requires provider packages (like `anthropic`, `openai`) to be installed separately.

**Solution:**
- Check for SDK availability before importing
- Provide clear error messages with installation instructions
- Document required dependencies
- Consider adding dependency checks to setup scripts

**Pattern:**
```python
try:
    from startd8.workflows.builtin.lead_contractor_workflow import LeadContractorWorkflow
except ImportError:
    print("Error: startd8 SDK not found.")
    print("Expected path: /path/to/startd8-sdk/src")
    print("Also ensure provider packages are installed:")
    print("  pip install anthropic openai")
    return error_result
```

**Key Takeaway:** External SDKs often have hidden dependencies. Document them clearly and check for them early.

---

## 2026-01-24: Value Capabilities Dashboard & Telemetry

### Tempo Instance Confusion

**Problem:** Data emitted to Docker Tempo (localhost:4317) was not visible in Grafana querying K8s Tempo (tempo.observability.svc.cluster.local:3200).

**Solution:**
- Verify which Tempo instance Grafana is configured to query
- Use port-forward to emit to K8s Tempo: `kubectl port-forward -n observability svc/tempo 4318:4317`
- Emit with explicit endpoint: `contextcore value emit --endpoint localhost:4318`

**Key Takeaway:** Always verify the data pipeline end-to-end - where data is emitted and where queries read from.

---

### Singular vs Plural Attribute Queries

**Problem:** TraceQL query `{ value.persona = "developer" }` returns 0 results even though developer appears in capabilities' persona lists.

**Solution:**
- `value.persona` contains only the PRIMARY persona
- `value.personas` contains comma-separated list of ALL personas
- Use regex for "any match": `{ value.personas =~ ".*developer.*" }`

**Pattern:**
```traceql
# Only primary persona match
{ .value.persona = "designer" }

# Any persona match (correct for filtering)
{ .value.personas =~ ".*developer.*" }
```

**Key Takeaway:** When an attribute has both singular and plural forms, the singular is typically the primary/default while plural contains the full list.

---

### Grafana Stat Panel Bug with TraceQL

**Problem:** Stat panels using TraceQL queries cause Grafana to hang with "Loading plugin panel..." and log nil pointer dereference errors.

**Solution:** Use table panels with `footer.countRows: true` instead of stat panels with `reduceOptions.calcs: ["count"]`.

**Error Pattern:**
```
level=error msg="Request error" error="runtime error: invalid memory address or nil pointer dereference"
stack="github.com/grafana/grafana/pkg/tsdb/tempo/search.go:102..."
```

**Key Takeaway:** When Grafana plugins have bugs, work around them with alternative visualization types rather than waiting for fixes.

---

### TraceQL Attribute Dot Prefix

**Problem:** TraceQL filter clause `value.type = "direct"` returns 0 results.

**Solution:** Span attributes require dot prefix in TraceQL filter clauses: `.value.type = "direct"`

**Pattern:**
```traceql
# Correct - dot prefix for span attribute
{ name =~ "value_capability:.*" && .value.type = "direct" }

# Wrong - missing dot prefix
{ name =~ "value_capability:.*" && value.type = "direct" }
```

**Key Takeaway:** TraceQL distinguishes between intrinsic span fields (no dot) and span attributes (dot prefix).

---

### Pydantic Enum Values Already Converted

**Problem:** `'str' object has no attribute 'value'` error when calling `.value` on enum fields.

**Solution:** Pydantic models with `model_config = ConfigDict(use_enum_values=True)` already convert enums to strings. Check type before calling `.value`:

**Pattern:**
```python
# Safe enum handling
audience = capability.get_audience()
audience_value = audience if isinstance(audience, str) else audience.value
span.set_attribute("capability.audience", audience_value)
```

**Key Takeaway:** When using Pydantic's `use_enum_values=True`, expect strings not enum instances.

---

## 2026-02-03: Phase 3 OTel Propagation (Wayfinder + ContextCore)

### OTel SDK Import Path Discovery (TraceContextTextMapPropagator)

**Problem:** `from opentelemetry.trace.propagation import TraceContextTextMapPropagator` fails with `ImportError` in OTel SDK 1.39.1. The class is not re-exported at the package level, despite being documented that way in older examples.

**Solution:** The correct import path is:
```python
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator
```

**Discovery Method:** Inspected the default global propagator's internal `_propagators` list to find the actual module path:
```python
from opentelemetry.propagate import get_global_textmap
propagator = get_global_textmap()
print(propagator._propagators)  # Reveals actual class locations
```

**Key Takeaway:** OTel Python SDK submodule re-exports vary across versions. When an import fails, inspect the runtime objects to find the real module path rather than guessing.

---

### Dual-Repo Contract Propagation Under Model C

**Problem:** When ContextCore (library) and Wayfinder (distribution) share contracts like `get_emit_mode()`, `RecordingRuleName`, and `AlertRuleName`, changes must be applied to both repos — but at different file locations and with different surrounding context.

**Solution:** Follow the Model C litmus test:
- **"Would a third-party developer need this?"** Yes = ContextCore. No = Wayfinder.
- Contracts, types, enums, and pure functions propagate to ContextCore.
- Deployment wiring (sampler factory, propagator setup, middleware) stays in Wayfinder.
- The same `get_emit_mode()` function exists at different line numbers in each repo's `otel_genai.py` (Wayfinder: ~line 54, ContextCore: ~line 891).

**Pattern for dual-repo edits:**
1. Implement and verify in Wayfinder first (deployment context available for integration testing).
2. Propagate only the library-level changes to ContextCore.
3. Run test suites in both repos independently.

**Key Takeaway:** In a library/distribution split, always edit the distribution first (more testable), then propagate contracts back to the library. Never propagate deployment wiring.

---

### validate_metric_name() vs kubernetes-mixin Naming Conventions

**Problem:** `validate_metric_name()` uses regex `^[a-z][a-z0-9_]*$` which rejects colons. But Prometheus recording rules follow the `level:metric:aggregation` pattern with colons (e.g., `project:contextcore_task_percent_complete:max_over_time5m`), and alert rules use CamelCase (e.g., `ContextCoreExporterFailure`).

**Solution:** Added separate validators for each naming convention:
- `validate_metric_name()` — regular metrics (lowercase, underscores only)
- `validate_recording_rule_name()` — recording rules (colons required, `level:metric:aggregation` pattern)
- `validate_alert_rule_name()` — alert rules (CamelCase, starts uppercase, no colons)

**Key Takeaway:** Prometheus has three distinct naming conventions (metrics, recording rules, alert rules). A single validator cannot handle all three — provide separate validators per convention.

---

### Idempotent Global Propagator Configuration

**Problem:** `configure_propagator()` sets the global OTel TextMapPropagator. It may be called from multiple initialization paths (`TaskTracker.__init__`, `A2AServer.__init__`), risking double-configuration or order-dependent bugs.

**Solution:** Use a module-level boolean flag for idempotency:
```python
_propagator_configured = False

def configure_propagator() -> None:
    global _propagator_configured
    if _propagator_configured:
        return
    propagator = CompositeHTTPPropagator([
        TraceContextTextMapPropagator(),
        W3CBaggagePropagator(),
    ])
    set_global_textmap(propagator)
    _propagator_configured = True
```

**Key Takeaway:** Any function that modifies OTel global state must be idempotent. Use a module-level guard since `set_global_textmap()` doesn't protect against double-calls itself.

---

### OTEL_SEMCONV_STABILITY_OPT_IN Precedence Design

**Problem:** Two env vars can control emit mode: the project-specific `CONTEXTCORE_EMIT_MODE` and the OTel standard `OTEL_SEMCONV_STABILITY_OPT_IN`. Need a clear precedence hierarchy that respects both without surprising behavior.

**Solution:** Three-tier resolution with explicit precedence:
1. `CONTEXTCORE_EMIT_MODE` (project-specific, highest priority)
2. `OTEL_SEMCONV_STABILITY_OPT_IN` (OTel standard, comma-separated tokens — `gen_ai_latest_experimental` maps to `EmitMode.OTEL`)
3. Default: `EmitMode.DUAL`

The project-specific var always wins because operators who set it have made a deliberate choice. The OTel standard var is a fallback for environments where only OTel-standard configuration is available.

**Key Takeaway:** When bridging project-specific and OTel-standard env vars, project-specific should take precedence — operators who explicitly set it have made a conscious decision.

---

## 2026-02-04: Demo Data Pipeline, Dependency Validation, Timestamp Clamping

### Loki Exporter Dead Code Crash

**Problem:** `load_to_loki()` crashed with `json.JSONDecodeError` on a stale code block (line 219) that attempted `json.loads()` on label keys. The correct implementation existed below but was unreachable.

**Solution:** Deleted the dead code block (lines 216-224). The correct string-splitting implementation at lines 226-241 then executed successfully.

**Key Takeaway:** When replacing code iteratively, delete the old implementation immediately. Leaving it "for reference" creates unreachable correct code shadowed by crashing stale code.

---

### Loki Future Timestamp Rejection and Belt-and-Suspenders Clamping

**Problem:** Demo data generated with `--months 0` produced timestamps in the future because task durations (5-30 days) exceeded the 24-hour window. Loki rejected with "timestamp too new."

**Solution:** Implemented two-layer clamping: (1) Generator clamps all timestamps to `now` ceiling with proportional duration scaling (`scale = total_days / 90.0`), (2) Consumer clamps any remaining future timestamps before push.

**Key Takeaway:** When pushing time-series data to systems with strict timestamp validation, clamp at both generation and ingestion. Neither layer alone is sufficient.

---

### Pre-Commit pass_filenames=false for Holistic Validation

**Problem:** Standard pre-commit hooks check only changed files. Dependency validation needs to scan ALL dashboards against the manifest to catch undeclared types.

**Solution:** Used `pass_filenames: false` with `files:` regex. The regex controls when the hook triggers; the script does a complete scan each time.

**Key Takeaway:** Use `pass_filenames: false` when validation requires cross-file consistency checking. Per-file hooks can't validate relationships between files and manifests.

---

## Template for Future Lessons

Use this format when adding new lessons:

---

### YYYY-MM-DD: [Topic/Feature Name]

**Problem:** [Describe the problem encountered]

**Solution:** [Describe the solution implemented]

**Example/Pattern:** [Code example if applicable]

**Key Takeaway:** [One sentence summary]

---
